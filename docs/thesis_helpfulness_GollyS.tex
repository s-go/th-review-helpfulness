\documentclass[
    %german,
    a4paper,%
    %11pt,%
    12pt,%
    oneside,%
    %twoside,%
    %titlepage,%
    %liststotoc,%
    toc=bibliography,
    %bibtotoc,%
    %headinclude,%
    %draft,
    final,
    %pointlessnumbers,%
    %fleqn,% mathematische Gleichungen linksbündig statt zentriert
]{scrartcl}

%\usepackage{polyglossia} % (neue) deutsche Beschriftungen und Silbentrennung
%\setdefaultlanguage[spelling=new]{german}
%\usepackage{times} % Nimbus Roman statt CM Serif
\usepackage{lmodern} % Latin Modern (in T1) statt CM
%%\usepackage[T1]{fontenc} % T1-Kodierung: Umlaute als *eine* Glyphe
%%\usepackage[utf8]{inputenc}
\usepackage{fontspec}
\setsansfont{Myriad Pro}
\setmainfont{Adobe Garamond Pro}

%%%%%%%%%%%%%%%% Seitenspiegel, Typografie %%%%%%%%%%%%%%%%%%%
%\usepackage{pdflscape} % Querformat: \begin{landscape}
\usepackage{geometry} % Seitenränder selbst bestimmen
\geometry{a4paper,%
          top=18mm,%
          left=30mm,%
          right=30mm,% ohne Marginalien: 20mm - mit Marginalien: 45mm
          bottom=22mm,%
          headsep=10mm,%
          footskip=12mm,%
         }
\setlength{\parindent}{0pt} % kein Einruecken bei Absatzbeginn
\setlength{\parskip}{8pt} % Absaetze durch Abstand kennzeichnen (1 Zeichenhoehe)

% Space between footnote mark and text
\usepackage[hang]{footmisc}
\setlength{\footnotemargin}{1em}

\usepackage{caption}
\captionsetup{
  labelfont=bf,
}

\usepackage{threeparttable}
\renewcommand{\TPTminimum}{\linewidth}

\usepackage{makecell} % \thead{} \makecell{}
\renewcommand\theadalign{cb}
\renewcommand\theadfont{\bfseries}

\usepackage{ulem} % durchgestrichener Text: \sout{}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor} % schönere Farben, z. B. RawSienna
\usepackage{enumerate} % Aufzählungsstil anpassen, z. B. {enumerate}[a)] – \setcounter{enumi}{4}

\usepackage{natbib} % \bibitem[Guevara(2010)]{Guevara2010} - \citet[5]{Guevara2010}
\bibpunct{(}{)}{;}{a}{}{,} % Interpunktion in Zitaten
\setcitestyle{notesep={: }} % Doppelpunkt zwischen Jahr und Seitenzahl

\usepackage{tocstyle}
\newtocstyle[KOMAlike][leaders]{alldotted}{}
\usetocstyle{alldotted}

%%%%%%%%%%%%%%%% Quelltext-Satz %%%%%%%%%%%%%%%%%%%
\usepackage{textcomp} % Text Companion fonts (für einfache Anführungszeichen)
\usepackage{listings} % Umgebung lstlisting; \lstinline$...$
\lstset{
	language=,                % the language of the code
	basicstyle=\ttfamily\small,
	xleftmargin=2em,
	xrightmargin=2em,
	captionpos=b,
	abovecaptionskip=.5em,
	commentstyle=\color{OliveGreen},% sets comment style
	tabsize=3,                      % sets default tabsize
	breaklines=true,                % sets automatic line breaking
	breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	escapechar=§,                   % escapes to LaTeX
	columns=flexible,               % columns=fixed / columns=flexible / columns=fullflexible
	upquote=true,                   % straight quotes
	literate={ö}{{\"o}}1            % national characters:
			 {ä}{{\"a}}1            %   *{replace}{replacement text}{length in output}
			 {ü}{{\"u}}1            %   * (optional): not in delimited text (strings, comments, ...) 
			 {Ö}{{\"O}}1
			 {Ä}{{\"A}}1
			 {Ü}{{\"U}}1
			 {ß}{{\ss}}2
	}

%%%%%%%%%%%%%%%%%%% Linguistik-Pakete %%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{mathtools} % includes amsmath, adds some nice fixes
\usepackage{amssymb} % für \varnothing
%\usepackage{semantic} % für |[ |]
%\usepackage{qtree} % qtree \Tree [. ] [. ] \qroof{}.
%\usepackage[x11names, rgb]{xcolor} % für dot2tex
%\usepackage{tikz} % für dot2tex
%\usetikzlibrary{arrows,shapes} % für dot2tex
         %%% unbedingt nach tikz-Paketen laden %%%
\usepackage{gb4e} % Beispiel-Umgebung: \begin{exe} \ex \begin{xlist}
%\usepackage{avm}             % für AVMs
%\avmfont{\sc}                % allgemeine AVM-Schriftart
%\avmvalfont{\it}             % Schriftart für Werte
%\avmsortfont{\footnotesize\it} % Schriftart für Sort Labels 

%%%%%%%%%%%%%%%%%%% hyperref %%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[hidelinks]{hyperref} % letzter Paketaufruf!
\makeatletter % changes the catcode of @ to 11
\AtBeginDocument{
  \hypersetup{ % hyperref: \title und \author in PDF-Eingenschaften übernehmen
    pdftitle = {\@title},
    pdfsubject = {\@subject},
    pdfauthor = {\@author}
  }
}
\makeatother % changes the catcode of @ back to 12

%%%%%%%%%%%%%%%%%%%%%%% Titel %%%%%%%%%%%%%%%%%%%%%%%%%%
\subject{Bachelor Thesis}
\title{Investigating the \\Impact of Discourse Relations\\ on Review Helpfulness}
\subtitle{}
%\setkomafont{author}{\normalsize}
\setkomafont{date}{\normalsize}
\setkomafont{publishers}{\normalsize}
\author{Sebastian Golly\\ {\normalsize (761737)}}
\date{\today}

% Double line spacing for text body, but not for titles
\usepackage[onehalfspacing]{setspace}
\addtokomafont{disposition}{\linespread{1}}

% No extra line spacing between list items
\usepackage{enumitem}
\setlist{nosep}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\vfill

\paragraph{Abstract}
This thesis investigates the influence individual discourse-relation types have on predicting the helpfulness of product reviews, and whether these effects vary across product categories. To this end, two probabilistic models are trained on electronics and book reviews, using the distributions of discourse-relation types as their only features. Based on feature-weight analyses and comparisons across product categories, two key results are drawn: (a)~Other than expected, not causal, but conjunction relations contribute most to predicting review helpfulness. (b)~Approximately half of the relation types exhibit similar effects on review helpfulness predictions in both product categories, while the other half shows differing effects.
\\[3em]

\vfill

\begin{center}
University of Potsdam\\[1em]
Department of Linguistics\\[1em]
Thesis committee:\\
Prof. Dr. Manfred Stede\\
Dr. Debopam Das
\end{center}

\thispagestyle{empty}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace*{\fill}

\paragraph{Zusammenfassung}

\begin{sloppypar}
Die vorliegende Arbeit untersucht den Einfluss verschiedener Typen von Diskursrelationen auf die automatische Bestimmung der Nützlichkeit von Produktrezensionen. Zusätzlich analysiert sie, ob sich diese Auswirkungen zwischen verschiedenen Produktkategorien unterscheiden. Hierzu werden zwei probabilistische Modelle auf Rezensionen elektronischer Produkte und Bücher trainiert, wobei die Verteilung von Diskursrelationen als einzige Gruppe von Merkmalen genutzt wird. Aus Analysen der Merkmalsgewichte und Vergleichen zwischen den Produktkategorien werden zwei wesentliche Erkenntnisse abgeleitet: (a)~Anders als erwartet steuern nicht kausale, sondern additive Relationen den größten Beitrag zur Vorhersage der Nützlichkeit bei. (b)~Für etwa die Hälfte der untersuchten Relationstypen lassen sich in beiden Produktkategorien ähnliche Auswirkungen auf die Nützlichkeitsbestimmung beobachten, während die andere Hälfte abweichende Effekte zeigt.
\end{sloppypar}

\thispagestyle{empty}
\pagebreak

\section{Introduction}

User-authored product reviews, published on the product pages of many e-commerce websites, have become an “important source of information for making informed purchase decisions” \citep[55]{Almagrabi2015}. These reviews typically include a rating of the customer's satisfaction with the product, usually expressed on a scale ranging from one star (“I hate it”) to five stars (“I love it”), as well as a free text in which the reviewer expresses, in an unstructured way, whatever they think is relevant to other prospective customers. Figure~\ref{fig:amazon-customer-review} gives an example of a typical product review on \textit{Amazon.com}.

\begin{figure}[h!]
\includegraphics[width=\textwidth]{img/amazon-customer-review}

\caption{Example product review posted on \textit{Amazon.com}.}
\label{fig:amazon-customer-review}
\end{figure}

Some reviews are more helpful to other users, while others are less. As website operators aim to display the most helpful reviews first in order to increase user engagement, they need a mechanism for ranking reviews according to their helpfulness. Currently, this is mainly achieved by asking users to assess the helpfulness of reviews written by other users (“Was this review helpful to you?”), aggregating this feedback, and using it as a ranking metric. However, this kind of manual assessment suffers from a number of limitations \cite[see][49f.]{Almagrabi2015}, including:

\begin{itemize}
\item \textit{Sparseness:} Many reviews have not received any helpfulness votes; many more have not received enough votes to reliably aggregate their helpfulness score.
\item \textit{No instant evaluation:} After a review has been posted, it takes a significant amount of time until a sufficient number of other users have evaluated it. As a consequence, it is impossible to immediately rank it according to its helpfulness.
\item \textit{Biases:} Human evaluations are subject to a number of biases. For instance, reviews with many positive helpfulness votes are displayed prominently, which results in being read by many users and receiving even more helpfulness votes. This “winner circle” makes it hard for new reviews to be ranked appropriately.
\end{itemize}

To overcome these limitations, it makes sense to find solutions for automatically evaluating review helpfulness. Probabilistic models specializing in this task will enable website operators to instantly obtain a helpfulness assessment for every single review, with less interference by human biases, and thus reliably rank reviews according to their helpfulness.

% ---

Various kinds of features have been proposed for training probabilistic models to predict review helpfulness. However, taking into account discourse relations (that is, descriptions that capture the logical connections holding between individual discourse segments, typically clauses) as an indicator for this task is a rather novel approach first explored by \citet{Mertz2014}. Recently, \citet{Golly2017} provides some evidence for the general claim that discourse structure influences the helpfulness of product reviews.

The goal of this thesis is to obtain a better understanding of how individual discourse-relation types affect review helpfulness. In addition, I want to investigate whether these effects vary across different product categories. To do so, two separate probabilistic models are trained on electronics and book reviews, using the distributions of discourse-relation types as their only features. Insights are drawn based on analyses of their feature weights and comparisons across product categories.

This thesis is structured as follows: Section~\ref{sec:background} provides background information on the PDTB framework of discourse structure. Section~\ref{sec:related-work} gives an overview of relevant previous work. Section~\ref{sec:goal} presents the research objectives and Section~\ref{sec:method} the methods applied for investigating them. The experimental results are described in Section~\ref{sec:results} and interpreted in Section~\ref{sec:discussion}. Finally, Section~\ref{sec:conclusion} briefly summarizes and concludes the study.


\section{Background}
\label{sec:background}

Different frameworks have been proposed for modeling discourse structure. Among the most widely used ones is PDTB (Penn Discourse Treebank 2.0), as presented in \citet{Prasad2008}. It has initially been applied to add a layer of discourse annotation to the Penn Treebank \citep{Marcus1993} and strives to be independent of any particular theory of discourse structure.

PDTB provides a \textit{lexically grounded} approach, that is, instances of discourse relations are usually signaled by a discourse connective. In PDTB, discourse is considered to exhibit a predicate-argument structure, with the connective operating as a binary predicate that takes two \textit{abstract objects} (AOs) as its arguments. AOs can be single clauses or sentences, or sequences of these. (\ref{ex:pdtb-basic}) \citep[taken from][3]{Webber2006} gives a basic example of the annotation, with the connective underlined, its first argument typeset in italics, and its second argument in bold.

\begin{exe}
\ex \label{ex:pdtb-basic} \textit{John eats porridge for breakfast}, \underline{while} \textbf{Mary eats muesli}.
\end{exe}

In case a relation holds between discourse units that is not explicitly signaled, an implicit connective is inferred and annotated, if possible. (\ref{ex:pdtb-implicit}) \citep[from][3]{Prasad2008} gives an example of such an \textit{implicit relation}.

\begin{exe}
\ex \label{ex:pdtb-implicit} But a few funds have taken other defensive steps. \textit{Some have raised their cash positions to record levels.} \underline{Implicit = \textsc{because}} \textbf{High cash positions help buffer a fund when the market falls.}
\end{exe}

PDTB only describes low-level relations between AOs, no relations between larger spans of text. Also, it does not try to cover the complete discourse, but only those parts of it where an explicit connective can be found or an implicit one can be inferred.

As discourse connectives are often ambiguous in meaning, PDTB provides an annotation of discourse-relation types, so-called \textit{senses}, used for disambiguation. These sense tags are structured in a three-level hierarchy where nested tags inherit the properties of their parents (see Figure~\ref{fig:pdtb-sense-hierarchy}). The subtypes \lstinline|Reason| and \lstinline|Result|, for instance, both inherit from the type \lstinline|Cause|, which is, in turn, a child of the \lstinline|Contingency| class.

\begin{figure}[h!]
\includegraphics[width=\textwidth, trim=3.5cm 17cm 3.5cm 2cm, clip]{img/pdtb-sense-hierarchy}

\caption{The PDTB hierarchy of sense tags. Reprinted from \citet[5]{Prasad2008}.}
\label{fig:pdtb-sense-hierarchy}
\end{figure}

(\ref{ex:since-1}) and (\ref{ex:since-2}) \citep[both taken from][4]{Prasad2008} show how the PDTB sense tags can be used to disambiguate between the temporal and causal readings of the connective \textit{since}.

\begin{exe}
\ex \label{ex:since-1} \textit{The Mountain View, Calif., company has been receiving 1,000 calls a day about the product} \underline{since} \textbf{it was demonstrated at a computer publishing conference several weeks ago}.\\
\hspace*{\fill}(\lstinline|Temporal.Asynchronous|)
\ex \label{ex:since-2} \textit{It was a far safer deal for lenders} \underline{since} \textbf{NWA had a healthier cash flow and more collateral on hand}.\hspace*{\fill}(\lstinline|Contingency.Cause|)
\end{exe}

The PDTB framework defines specific relation types for \textit{pragmatic} uses of connectives. While standard relations are fact-related, their pragmatic counterparts are rather presentational. For instance, the type \lstinline|Contingency.Cause| is used when there is a reason-result relationship between the situations described in both arguments. \lstinline|Contingency.Pragmatic Cause|, on the other hand, is annotated when one of the arguments provides justification for a claim made in the other one, without a causal influence between the two situations \citep[see][29]{Prasad2007}.

% Lin
\label{para:pdtb-parser}
With their \textit{PDTB-styled end-to-end discourse parser}, \citet{Lin2014} present a tool for automatically annotating free text with PDTB sense tags. It implements a pipeline consisting of a connective classifier, an argument labeler, an explicit classifier, a non-explicit classifier, as well as an attribution-span labeler, trained on the PDTB corpus. They report “an overall system $ F_1 $ score of 46.80 percent for partial matching utilizing gold standard parses, and 38.18 percent with full automation” \citep[152]{Lin2014}, where “a large portion of the misses comes from the Non-Explicit relations, as these are more difficult to classify in comparison with the Explicit relations”  \citep[177]{Lin2014}. The component classifying explicitly signaled relations achieves a high $ F_1 $ performance of 80.61 percent (with full automation and error propagation), while the non-explicit classifier suffers from an unsatisfactory $ F_1 $ score of 25.46 percent.

\section{Related Work}
\label{sec:related-work}

There have been a number of studies working towards the goal of automatically assessing the helpfulness of product reviews by training probabilistic models on corpora that include manual helpfulness evaluations. Many of them see the problem of predicting review helpfulness as a regression task in which the model learns to assign a helpfulness score to each review (see, e.g., \citealt{Kim2006}, \citealt{ZhangVaradarajan2006}, and \citealt{SieringMuntermann2013}). Others treat it as the problem of classifying each review, mostly into one of the two classes \textit{helpful} and \textit{not helpful} (see, e.g., \citealt{ChenTseng2011}, \citealt{Mertz2014}, and \citealt{Zhang2015}). For an extensive survey of the different approaches, see \citet{Almagrabi2015}.

\citet[186]{MudambiSchuff2010} define review helpfulness as “a measure of perceived value in the decision-making process” of a consumer. They show that product type has an influence on how the helpfulness of reviews is evaluated. Basing on a classification introduced by \citet{Nelson1970}, they distinguish between \textit{search goods} whose relevant properties can easily be compared without necessarily having to interact with the product (e.g., laser printers, computer screens, or tires) and \textit{experience goods} whose key attributes are subjective and thus difficult to evaluate prior to interaction (e.g., music CDs, headphones, or wine). They find that for experience goods, but not for search goods, reviews that give a moderate rating are more helpful than reviews with a strongly positive or a strongly negative rating. In addition, they discover that the positive correlation between the length of reviews and their helpfulness is stronger for search goods than for experience goods.

% Kim
In an influential study, \citet{Kim2006} propose a Support Vector Machine (SVM) regression model for predicting review helpfulness. They systematically investigate how well different types of features capture the helpfulness of a review and evaluate different combinations of these features, including

\begin{itemize}
\item \textit{structural} features (e.g., the number of tokens in the review),
\item \textit{lexical} features (e.g., its \textit{n-gram} statistics),
\item \textit{syntactic} features (e.g., part-of-speech analyses),
\item \textit{semantic} features (e.g., sentiment analyses), and
\item \textit{meta-data} features (e.g., the overall product rating the review gives).
\end{itemize}

The feature combination that performs best in their experiments captures the length of the review, its unigram statistic, and its product rating.

% Mertz
\citet{Mertz2014} argue that the main influence on review helpfulness is the text of the review itself (rather than its product rating or other features derived from meta data). Accordingly, they propose and evaluate two novel text-based features for predicting review helpfulness: dependency bigrams and discourse connectives. However, their reasoning that a “potential indicator of review helpfulness could be the amount of internal discourse in a review text” \citep[7]{Mertz2014} reveals a theoretical shortcoming in their work: There is no “amount of discourse” in a text, since every text constitutes a piece of discourse.

Building upon a baseline model including star-rating and unigram features, they add features capturing the frequencies of discourse connectives aggregated into PDTB senses. As they do not use discourse parsing but simple regular-expression matching for extracting discourse features, it remains unclear, however, how they disambiguate between the senses of a connective. Also, they set up their experiment in a way that the baseline model already obtains an extremely high classification performance, which can explain why none of the newly proposed features significantly improves prediction performance.

% Golly
A recent study by \citet{Golly2017} takes up the idea of \citet{Mertz2014} that discourse structure is likely to have an effect on the perceived helpfulness of product reviews, while trying to overcome the shortcomings in their work. It enriches a baseline regression model using the best-performing feature combination from \citet{Kim2006} by features capturing the presence and occurrence frequencies of different discourse-relation types. To this end, it employs the discourse parser by \citet{Lin2014} to extract explicit discourse connectives and aggregate them into PDTB senses. The obtained results support the hypothesis that the distribution of discourse relations in a product review is an indicator of its helpfulness to other users – at least if the term “distribution” is defined as the \textit{presence} of certain relation types, rather than their occurrence frequencies. However, as this experiment uses an SVM regression model with a non-linear RBF kernel that does not allow for feature-weight analysis, the contributions of individual discourse-relation types on helpfulness predictions could not be investigated.

\section{Goal of this Study}
\label{sec:goal}

\citet{Golly2017} has shown that there is an effect of the distribution of discourse relations in product reviews on their perceived helpfulness to other users. It is still unclear, however, how certain discourse-relation types affect review helpfulness. The present thesis aims to be a first step towards filling this research gap by investigating two questions:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q1}] What is the effect of individual discourse-relation types on predicting review helpfulness?
\end{enumerate}

I expect product reviews that do not only make claims, but justify and reason these claims to be more comprehensible and credible to other users, offering an increased value in their purchase-decision process, and, consequently, being perceived as more helpful.

\textbf{Example:}
\vspace{-1em}

\begin{exe}
\ex Personal video glasses are a difficult item to purchase \textit{(Claim)}
\ex Personal video glasses are a difficult item to purchase because it’s very hard to find a store that carries them \textit{(Justified claim)}
\end{exe}
\vspace{-.5em}

This expectation can be phrased as a first hypothesis:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H1}] Causal relations (\lstinline|Contingency.Cause| and \lstinline|Contingency.Pragmatic Cause|, in terms of PDTB sense tags) have a particularly high impact on review helpfulness.
\end{enumerate}

The second research question considers potential distinctions between product categories. \citet{MudambiSchuff2010} find that reviews on different product types are evaluated differently with respect to their helpfulness. They report both the effects of star ratings and review lengths to vary between search and experience goods. Basing on this observation, I want to investigate whether the effects of discourse structure vary across product categories as well, e.g., when comparing electronic products to books.

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q2}] Does the impact of discourse-relation types on review helpfulness vary across product categories?
\end{enumerate}

As reviews of search goods tend to focus on “objective claims about tangible attributes” \citep[189]{MudambiSchuff2010} whereas reviews of experience goods generally include more “subjective evaluations” (ibid.) and consumers seem to assess review helpfulness differently for these product types, I assume this dissimilarity to reflect in the effect of discourse relations, as well. This leads to the second hypothesis of this study:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H2}] The effect of individual discourse-relation types on predicting review helpfulness differs among different product categories.
\end{enumerate}

\section{Method}
\label{sec:method}

For investigating the research questions presented in the previous section, two probabilistic models have been trained on reviews from different product categories. This section describes the datasets used in the experiments, the learning task and features of the probabilistic models, as well as the overall experimental setup.

\subsection{Data}

From the 5-core corpus of \textit{Amazon.com} reviews created by \citet{HeMcAuley2016},\footnote{Available at \url{http://jmcauley.ucsd.edu/data/amazon/}.} two product categories have been chosen: “Electronics” and “Books”. While the “Electronics” category includes mostly search goods (such as printers), the “Books” category contains many experience goods (such as fiction books).

Each of the reviews in the two datasets includes, among others:

\begin{itemize}
\item the text of the review,
\item its overall product rating (1 to 5 stars),
\item the numbers of positive and total helpfulness votes it has received.
\end{itemize}

Listing~\ref{sample-review} shows a shortened sample review from the corpus.

\begin{lstlisting}[basicstyle=\ttfamily\small\singlespacing, caption=Sample review (shortened) from the “Electronics” category., label=sample-review, float]
{
    "asin": "B000W9OJVA",
    "helpful": [
        103,
        105
    ],
    "overall": 4.0,
    "reviewText": "I recently purchased a set of these. Personal video glasses are a difficult item to purchase because it's very hard to find a store that carries them, let alone lets you try them on before purchase, so I wasn't quite sure what to expect. [...]",
    "reviewTime": "03 27, 2008",
    "reviewerID": "AVPNQUVZWMDSX",
    "reviewerName": "esanta \"esanta\"",
    "summary": "Fantastic!",
    "unixReviewTime": 1206576000
}
\end{lstlisting}

The review texts have been preprocessed to facilitate sentence-boundary detection, as many reviews missed whitespace after sentence-final punctuation or exhibited a use of ellipses instead of periods.

In order to use only informative items, reviews with less than ten helpfulness votes or with a text shorter than 20 characters have been filtered out. For each of the two product categories, 20,000 reviews have been randomly sampled and partitioned into a development dataset containing 2,000 reviews and a cross-validation dataset containing 18,000 reviews.\footnote{Available at \url{https://drive.google.com/open?id=0B4FHGozCmQFEeTQtMUEzLWpLcTA}.}

\subsection{Learning Task}

Helpfulness prediction is sometimes treated as a classification problem, labeling each review as either \textit{helpful} or \textit{not helpful} \citep[see][3ff.]{Almagrabi2015}. In order to allow for ranking reviews according to their helpfulness, however, it makes sense to rather perceive the problem as a regression task. Given a product review and its metadata, the probabilistic model predicts a score that captures the helpfulness of the review.

For learning the model, the helpfulness score of a product review can be simply defined as the ratio of its positive helpfulness votes to its total helpfulness votes. \citet[424]{Kim2006} formalize this metric as the helpfulness function $ h $ that computes the helpfulness score of a review $ r $:

\vspace{-2em}

\begin{align}
\label{helpfulness-fuction}
h(r \in R) = \frac{rating_+(r)}{rating_+(r) + rating_-(r)}
\end{align}

This helpfulness ratio is defined for every review with at least one helpfulness vote. It ranges between 0 (minimum helpfulness) and 1 (maximum helpfulness). For training and evaluation, the gold standard of helpfulness scores is determined based on the helpfulness votes given by \textit{Amazon.com} users that are indicated in the corpus.

\subsection{Features}

In order to answer the research questions, the review texts had to be annotated with discourse relations. PDTB sense tags have been extracted automatically using the \textit{PDTB-styled end-to-end discourse parser} \citep{Lin2014}. It can be configured to extract sense tags at any of the three levels of the PDTB sense hierarchy (see page \pageref{fig:pdtb-sense-hierarchy}). As a reasonable compromise between accuracy and parsing performance, sense tags from the second (\textit{type}) level have been extracted (e.g., \lstinline|Comparison.Contrast| or \lstinline|Contingency.Cause|). Because of the unsatisfactory performance of the non-explicit classifier (see page \pageref{para:pdtb-parser}), only explicitly signaled relations have been used in the experiments.

\begin{sloppypar}
Spot tests showed that the discourse parser was unable to correctly distinguish pragmatic PDTB types (\lstinline|Contingency.Pragmatic cause|, \lstinline|Contingency.Pragmatic condition|, \lstinline|Comparison.Pragmatic contrast|, and \lstinline|Comparison.Pragmatic concession|) from their non-pragmatic counterparts. As this distinction is not crucial to the intended analyses, I decided to merge the pragmatic types with their non-pragmatic variants. That is, the \lstinline|Contingency.Cause| feature also includes all instances of \lstinline|Contingency.Pragmatic cause| relations, covering both purely causal and justifying relations.
\end{sloppypar}

(\ref{ex:pdtb-types-first}–\ref{ex:pdtb-types-last}) show typical examples for each of the extracted PDTB types, with the respective discourse connectives underlined.

\begin{exe}
\ex\label{ex:pdtb-types-first} \lstinline|Temporal.Asynchronous|\\
The internal cables could be a tiny bit longer, so measure carefully inside your case \underline{before} you purchase

\ex \lstinline|Temporal.Synchrony|\\
the DVD sounded like it was going through a meat grinder \underline{when} it was playing

\ex \lstinline|Contingency.Cause|\\
i got this lens \underline{because} i wanted a compact wide angle lens that i could easily throw in my bag to cover events.

\ex \lstinline|Contingency.Condition|\\
So \underline{if} you want a great 70-200 zoom and crave attention and can carry the weight this lens is for you.

\ex \lstinline|Comparison.Contrast|\\
Yeah, you'll pay a little more, \underline{but} the quality you'll get will be worth it.

\ex \lstinline|Comparison.Concession|\\
\underline{Although} the camera cannot zoom very far it still is a very good camera.

\ex \lstinline|Expansion.Conjunction|\\
The kit lenses are a great starter. \underline{And} they're worlds better than anything you'll get with a point-and-shoot.

\ex \lstinline|Expansion.Instantiation|\\
The only problem i have with the camera is that it has a hard time focusing on small texts. \underline{For example} i want to copy a receipt and send it to the manufacturer it would come out blurry.

\ex \lstinline|Expansion.Restatement|\\
\underline{Overall}, I am happy with the build and performance of these chargers.

\ex \lstinline|Expansion.Alternative|\\
you need to press it slow and firm, \underline{or} it won't start.

\ex \lstinline|Expansion.Exception|\\
wifi works well, \underline{except} that streaming does not work all the time.

\ex\label{ex:pdtb-types-last} \lstinline|Expansion.List|\\
The keyboard is great, screen is good \underline{and} overall quality is very good.
\end{exe}

Based on the extracted occurrences of discourse relations in each review, three variants of discourse-relation features have been aggregated:

\begin{itemize}
\item \lstinline|REL-CNT|: occurrence frequencies of explicit discourse-relation types in each review (listing \ref{feat-rel-cnt} shows an instance of this feature set for a sample review),
\item \lstinline|REL-IPT|: normalized frequencies of explicit discourse-relations types (as instances per token),
\item \lstinline|REL-PRS|: presence of explicit discourse-relation types in each review (with values \lstinline|1| or \lstinline|0| according to whether or not a certain discourse-relation type occurs in the review).
\end{itemize}

\begin{lstlisting}[caption=\texttt{REL-CNT} feature for a sample review., label=feat-rel-cnt, basicstyle=\ttfamily\small\singlespacing]
  {
      'Comparison.Concession': 0,
      'Comparison.Contrast': 13,
      'Comparison.Pragmatic concession': 0,
      'Comparison.Pragmatic contrast': 0,
      'Contingency.Cause': 8,
      'Contingency.Condition': 2,
      'Contingency.Pragmatic condition': 0,
      'Expansion.Alternative': 0,
      'Expansion.Conjunction': 17,
      'Expansion.Exception': 0,
      'Expansion.Instantiation': 0,
      'Expansion.List': 0,
      'Expansion.Restatement': 0,
      'Temporal.Asynchronous': 8,
      'Temporal.Synchrony': 6
  }
\end{lstlisting}

For training and evaluating the models, each feature was scaled to the range $ [-1, 1] $.


\subsection{Experimental Setup}

Similar to \citet{Kim2006} and \citet{Golly2017}, a Support Vector Machine (SVM) regression model has been trained to predict review helpfulness. However, as the feature weights of the non-linear RBF kernel that has been used in previous studies cannot be analyzed meaningfully, a linear kernel has been chosen instead. Pre-studies showed that, with thorough tuning of the hyperparameters $ C $ and $ \epsilon $ (performing a full-grid search on the development dataset), an equal level of prediction performance can be reached even with the less powerful linear kernel.

In contrast to \citet{Golly2017} that used discourse-relation features in addition to a set of baseline features (overall rating, review length, and unigram statistics), the models in this study rely only on discourse-relation features in order to eliminate cross-effects with other factors.

For the purpose of comparing feature weights between product categories, separate models have been trained on reviews from the two datasets (“Electronics” and “Books”).

For evaluating the results, ten-fold cross-validation has been applied, where each model was trained using nine folds, and its performance was evaluated on the remaining test fold. As an evaluation metric, the Pearson correlation coefficient (\textit{Pearson's r}) between the predicted helpfulness scores and the gold standard (based on the \textit{Amazon.com} helpfulness votes) has been computed. It ranges between -1 (total negative correlation) and +1 (total positive correlation).

As a result of a pre-study, Table~\ref{tab:disc-features-performance} compares the performance of the three discourse-feature variants presented in the previous subsection. In either of the two product categories, \lstinline|REL-PRS| that captures the presence of each discourse-relation type in a review yields the best performance. For this reason, it was selected for training and evaluating the models.

\begin{table}[h!]
	\centering
	
	\caption{Evaluation results of a linear SVR model using different variants of discourse-relation features}
	\label{tab:disc-features-performance}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{Feature} & \thead{Electronics\\ Pearson's \textit{r}\tnote{a}} & \thead{Books\\ Pearson's \textit{r}\tnote{a}} \\ \hline
		\lstinline|REL-CNT| & 0.211 (± 0.040) & 0.167 (± 0.058) \\ \hline
		\lstinline|REL-IPT| & 0.086 (± 0.040) & 0.125 (± 0.064) \\ \hline
		\lstinline|REL-PRS| & 0.279 (± 0.056) & 0.237 (± 0.084) \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
	\centering
	\footnotesize
	\item[a] 95\% confidence bounds are calculated using 10-fold cross-validation.
	\end{tablenotes}
	
	\end{threeparttable}

\end{table}

\section{Results}
\label{sec:results}

\begin{sloppypar}
This section presents the results with regard to both research questions. They show that the discourse-relation type that contributes most to predicting review helpfulness is \lstinline|Expansion.Conjunction|, with causal relations having substantially lower impact. When comparing the results between both product categories, around half of the relation types exhibit similar impact on review helpfulness, while the other half shows differing effects.
\end{sloppypar}

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q1}] What is the effect of individual discourse-relation types on predicting review helpfulness?
\end{enumerate}

Tables~\ref{tab:feature-weights-electronics} and \ref{tab:feature-weights-books} rank the features of both probabilistic models (for electronics and book reviews) according to their learned regression coefficients, or feature weights.
The coefficients represent the mean change in the predicted helpfulness scores if the corresponding discourse-relation type is present in a review (while holding the other predictors in the model constant). Positive coefficients indicate that the presence of the discourse relation increases on average the predicted helpfulness score, while negative coefficients mark a negative impact on the predicted helpfulness.

\begin{table}[h!]
	\centering
	
	\caption{Ranking of discourse-relation types according to their feature-weight coefficients in the model trained on electronics reviews}
	\label{tab:feature-weights-electronics}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{clr}
		\textbf{Rank} & \textbf{PDTB Sense Tag} & \textbf{Coefficient}\tnote{\textit{a}} \\ \hline
		1 & Expansion.Conjunction & 0.048 \\ \hline
		2 & Temporal.Synchrony  & 0.023 \\ \hline
		3 & Comparison.Contrast\tnote{\textit{b}} & 0.018 \\ \hline
		4 & Contingency.Condition\tnote{\textit{b}}  & 0.012 \\ \hline
		5 & Contingency.Cause\tnote{\textit{b}}  & 0.011 \\ \hline
		6 & Comparison.Concession\tnote{\textit{b}} & 0.009 \\ \hline
		7 & Expansion.Restatement  & 0.008 \\ \hline
		8 & Expansion.List  & 0.004 \\ \hline
		9 & Temporal.Asynchronous    & 0.001 \\ \hline
		10 & Expansion.Instantiation & -0.000 \\ \hline
		11 & Expansion.Exception  & -0.001 \\ \hline
		12 & Expansion.Alternative  & -0.003 \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
	\centering
	\footnotesize
	\item[\textit{a}] Coefficients calculated using 10-fold cross-validation.
	\item[\textit{b}] Including \textit{Pragmatic} type.
	\end{tablenotes}
	
	\end{threeparttable}
\end{table}

\begin{table}[h!]
	\centering
	
	\caption{Ranking of discourse-relation types according to their feature-weight coefficients in the model trained on book reviews}
	\label{tab:feature-weights-books}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{clr}
		\textbf{Rank} & \textbf{PDTB Sense Tag} & \textbf{Coefficient}\tnote{\textit{a}} \\ \hline
		1 & Expansion.Conjunction & 0.093 \\ \hline
		2 & Temporal.Synchrony  & 0.028 \\ \hline
		3 & Comparison.Contrast\tnote{\textit{b}} & 0.017 \\ \hline
		4 & Expansion.Instantiation & 0.015 \\ \hline
		5 & Temporal.Asynchronous    & 0.013 \\ \hline
		6 & Comparison.Concession\tnote{\textit{b}} & 0.007 \\ \hline
		7 & Expansion.Restatement  & 0.000 \\ \hline
		8 & Contingency.Condition\tnote{\textit{b}}  & -0.006 \\ \hline
		9 & Contingency.Cause\tnote{\textit{b}}  & -0.006 \\ \hline
		10 & Expansion.Alternative  & -0.025 \\ \hline
		11 & Expansion.Exception  & -0.033 \\ \hline
		12 & Expansion.List  & -0.041 \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
	\centering
	\footnotesize
	\item[\textit{a}] Coefficients calculated using 10-fold cross-validation.
	\item[\textit{b}] Including \textit{Pragmatic} type.
	\end{tablenotes}
	
	\end{threeparttable}
\end{table}

For both product categories, the presence of explicit \lstinline|Expansion.Conjunction| relations proves to be by far most predictive of review helpfulness. This discourse-relation type is typically expressed by connectives such as \textit{and} or \textit{also}.

As this result is rather unexpected, it deserves a more detailed investigation. (\ref{ex:conjunction-first}–\ref{ex:conjunction-last}) show typical examples for instances of \lstinline|Expansion.Conjunction| relations in the corpus. The connectives signaling the relation are underlined. It can be observed that, in many of the examples, the conjunction relation is used to present an additional argument (in terms of argumentation structure), often for buying the reviewed product.

\begin{exe}
\ex \label{ex:conjunction-first} The kit lenses are a great starter. \underline{And} they're worlds better than anything you'll get with a point-and-shoot.
\ex Plenty of reviews here on how great the camera is, \underline{and} I agree with them.
\ex You're not just buying an e500 here, you're \underline{also} buying into Olympus's lens system.
\ex The image quality is on par with similar offerings from Nikon and Canon, \underline{and} it has the same, or very similar features.
\ex Spend a little more now, \underline{and} it'll literally save you about \$200 later.
\ex The 14-54 is built like a tank \underline{and} the image quality will blow you away.
\ex \label{ex:conjunction-last} Overall, the sets are very similar, my personal choice being the Haier, as I think the picture is sharper, the price is better and the battery lasts longer for my needs in bad weather. Haier \underline{also} has way more picture settings and adjustments in the tv's menu
\end{exe}

Samples of reviews without any instance of an \lstinline|Expansion.Conjunction| relation often exhibit one or more of the following properties:

\begin{itemize}
\item short overall length,
\item short sentence length,
\item low usage of cohesive devices.
\end{itemize}

(\ref{ex:no-conjunction}) gives a complete example of one of the reviews showing all of these properties.

\begin{exe}
\ex \label{ex:no-conjunction} It works fine.  It was easy to install.  I think that may have something to do with my experience with other routers though.  I am new to this.
\end{exe}

In order to find out whether these indications gained from individual samples generalize over the complete corpus, I analyzed the following features and compared their average values between reviews including and reviews without \lstinline|Expansion.Conjunction| relations:

\begin{itemize}
\item review length: overall number of tokens,
\item sentence length: average number of tokens per sentence,
\item use of connectives: instances of explicit discourse relations per thousand tokens,
\item helpfulness score.
\end{itemize}

Tables~\ref{tab:conjunction-features-electronics} and \ref{tab:conjunction-features-books} show that, for both product categories, reviews including at least one instance of an explicit \lstinline|Expansion.Conjunction| relation have on average both higher total and sentence lengths, higher normalized frequencies of explicitly signaled discourse relations, as well as higher helpfulness ratings.


\begin{table}[h!]
	\centering
	
	\caption{Comparison of selected linguistic features between reviews including and reviews without \lstinline|Expansion.Conjunction| relations in electronics reviews}
	\label{tab:conjunction-features-electronics}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{} & \thead{Reviews including\\ Expansion.Conjunction} & \thead{Reviews without\\ Expansion.Conjunction} \\ \hline
		Avg. review length & 364.33 & 101.84 \\ \hline
		Avg. sentence length & 17.64 & 14.95 \\ \hline
		Avg. use of connectives & 38.34 & 23.36 \\ \hline
		Avg. helpfulness score & 0.83 & 0.71 \\ \hline
		\end{tabular} 
	}
	
	\end{threeparttable}

\end{table}


\begin{table}[h!]
	\centering
	
	\caption{Comparison of selected linguistic features between reviews including and reviews without \lstinline|Expansion.Conjunction| relations in book reviews}
	\label{tab:conjunction-features-books}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{} & \thead{Reviews including\\ Expansion.Conjunction} & \thead{Reviews without\\ Expansion.Conjunction} \\ \hline
		Avg. review length & 321.61 & 108.77 \\ \hline
		Avg. sentence length & 19.9 & 16.53 \\ \hline
		Avg. use of connectives & 31.5 & 19.27 \\ \hline
		Avg. helpfulness score & 0.77 & 0.65 \\ \hline
		\end{tabular} 
	}
	
	\end{threeparttable}

\end{table}

Contrary to the first hypothesis, the presence of \lstinline|Contingency.Cause| relations does not show to be a particularly good predictor of review helpfulness. In electronics reviews, the corresponding feature obtains the fifth rank according to its feature weight. In the model trained on book reviews, it even gets assigned a slightly negative coefficient and only reaches rank nine.

When analyzing samples of the automatically extracted relations, a noticeable number of parsing mistakes with respect to \lstinline|Contingency.Cause| can be observed. (\ref{ex:parsing-mistakes-first}–\ref{ex:parsing-mistakes-last}) present typical examples of such incorrect annotations that frequently occur.

\begin{exe}
\ex \label{ex:parsing-mistakes-first} Annotation of \lstinline|Temporal.Asynchronous| instead of \lstinline|Contingency.Cause|\\
Don't know if the problem is router related or n-version related but \underline{since} range and not speed is the issue for me, I do not find the unit to be an improvement over what I have already installed.
\ex Annotation of \lstinline|Temporal.Synchrony| instead of \lstinline|Contingency.Cause|\\
The picture is absolutely gorgeous, \underline{as} it has the new atsc digital signal receiver built in!
\ex \label{ex:parsing-mistakes-last} Annotation of \lstinline|Contingency.Cause| instead of \lstinline|Expansion.Conjunction|\\
I've had time to thoroughly test it \underline{and} I've sort of begun to have second thoughts about these.
\end{exe}

In sum, the results of the feature-weight analyses do not support the first hypothesis that expected a strong impact of \lstinline|Contingency.Cause| relations on predicting review helpfulness. The analyses of corpus samples and linguistic features, however, provide a number of findings that will be referred to in the discussion section to account for the observed results.

%%% Q2

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q2}] Does the impact of discourse-relation types on review helpfulness vary across product categories?
\end{enumerate}

Table~\ref{tab:rank-comparison} compares the ranks of discourse-relation types according to their respective feature weights between the two product categories. Substantial differences are highlighted in bold.

\begin{table}[h!]
	\centering
	
	\caption{Comparison of discourse-relation ranks between models trained on electronics vs. book reviews}
	\label{tab:rank-comparison}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{PDTB Sense Tag} & \thead{Electronics Rank\\ (Coefficient)} & \thead{Books Rank\\ (Coefficient)} \\ \hline
		Expansion.Conjunction & 1 (0.048) & 1 (0.093) \\ \hline
		Temporal.Synchrony  & 2 (0.023) & 2 (0.028) \\ \hline
		Comparison.Contrast\tnote{\textit{a}} & 3 (0.018) & 3 (0.017) \\ \hline
		Contingency.Condition\tnote{\textit{a}}  & \textbf{4 (0.012)} & \textbf{8 (-0.006)} \\ \hline
		Contingency.Cause\tnote{\textit{a}}  & \textbf{5 (0.011)} & \textbf{9 (-0.006)} \\ \hline
		Comparison.Concession\tnote{\textit{a}} & 6 (0.009) & 6 (0.007) \\ \hline
		Expansion.Restatement  & 7 (0.008) & 7 (0.000) \\ \hline
		Expansion.List  & \textbf{8 (0.004)} & \textbf{12 (-0.041)} \\ \hline
		Temporal.Asynchronous    & \textbf{9 (0.001)} & \textbf{5 (0.013)} \\ \hline
		Expansion.Instantiation & \textbf{10 (-0.000)} & \textbf{4 (0.015)} \\ \hline
		Expansion.Exception  & 11 \textbf{(-0.001)} & 11 \textbf{(-0.033)} \\ \hline
		Expansion.Alternative  & \textbf{12 (-0.003)} & \textbf{10 (-0.025)} \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
		\centering
		\footnotesize
		\item[\textit{a}] Including \textit{Pragmatic} type.
	\end{tablenotes}
	
	\end{threeparttable}
\end{table}

Half of the examined discourse-relation types exhibit similar results in both product categories. This includes the three relation types with the strongest positive coefficients that have identical ranks for both electronic products and books. For the remaining relation types, a number of differences can be observed: 

\begin{enumerate}
\item Both types of the \lstinline|Contingency| class (\lstinline|Condition| and \lstinline|Cause|) are positive predictors for electronics reviews, while having small negative coefficients in the model trained on book reviews.
\item \lstinline|Temporal.Asynchronous| and \lstinline|Expansion.Instantiation| relations have clearly positive coefficients in the books domain, while showing no predictive power for electronics reviews.
\item The three \lstinline|Expansion| types \lstinline|List|, \lstinline|Exception|, and \lstinline|Alternative| exhibit a clear negative correlation with the helpfulness of book reviews while having almost no effect in the electronics domain.
\end{enumerate}

With regard to the second hypothesis, the results give a split answer. While one half of the discourse-relation types shows a similar impact on predicting review helpfulness, the other half exhibits differing effects in the two examined product categories.

\section{Discussion}
\label{sec:discussion}

The experimental results do not support the expecation stated in the first hypothesis:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H1}] Causal relations (\lstinline|Contingency.Cause| and \lstinline|Contingency.Pragmatic Cause|, in terms of PDTB sense tags) have a particularly high impact on review helpfulness.
\end{enumerate}

\begin{sloppypar}
Instead, the relation that has shown to have the highest impact on review helpfulness is \lstinline|Expansion.Conjunction|. There are different potential accounts for this unexpected and even counter-intuitive result.
\end{sloppypar}

One of them can be dubbed \textit{multi-confirmation} account. As “[c]onsumers follow a purchase decision process that seeks to reduce uncertainty” \citep[187]{MudambiSchuff2010}, they read product reviews, at least partly, as a source of confirmation. Once they are interested enough in a product to read its reviews, many of them only seek for an endorsement to make the last step and purchase it. The more confirming statements a review contains, the more helpful it will be perceived by these users. An examination of samples of conjunction relations in the corpus showed that the relation is often used to present an additional argument for buying the reviewed product. Following the multi-confirmation interpretation, these uses of conjunction relations increase the helpfulness of the corresponding review.

A second account refers to the \textit{writing quality} of the examined reviews. \citet{Liu2008} find that writing style has an influence on review-helpfulness predictions. The analyses performed in this study show that the presence of \lstinline|Expansion.Conjunction| relations is positively correlated with linguistic features such as review length, average sentence length, and density of discourse connectives. The greater mean values of these features observed in reviews including at least one instance of a conjunction relation might indicate an increased writing quality of these reviews on average which would explain their gain in perceived helpfulness.

A third and last account considers findings from argumentation-structure research. \citet[2239]{Eckle-Kohler2015} identify the German discourse connective \textit{und} (\textit{and}) that signals conjunction relations as being indicative of \textit{premises}. In the claim-premise model of argumentation they adopt, premises are used for supporting (or attacking) a claim. This is in line with the initial expectation that lead to the first hypothesis: the assumption that justified claims (supported by one or more premises) are more comprehensible and credible to other users and thereby increase review helpfulness.

% Accounts for medium/low predictiveness of causal relations

Besides the high predictiveness of \lstinline|Expansion.Conjunction| relations, the observed medium to low predictiveness of causal relations deserves a distinct interpretation. Again, the results can be accounted for from different perspectives.

As mentioned before, the assumption that a high ratio of justified claims gives rise to an increase in helpfulness might be better captured by analyses of argumentation structure rather than discourse structure. In fact, \citet[54]{StabGurevych2014} point out that “approaches relying on discourse markers are not applicable for identifying argumentative discourse structures in documents which do not follow a standardized form.” Thus, it could be promising to investigate this assumption based on an argumentation-mining approach.

Second, it has to be stressed that for the analyses performed in this study, only explicit discourse relations have been taken into account because of the poor performance of current discourse parsers in automatically detecting implicit relations. The expectations that underlie the first hypothesis focus on \textit{subjective causality} (supporting a claim rather than relating two facts). If we classify product reviews as a rather persuasive than informative text type, the findings of \citet{Kamalski2008} provide another explanation to the low impact of causal relations. They find that, in a persuasive context, explicitly marked subjective relations trigger resistance in the readers and thereby reduce the persuasuasive power of the text. In any case, ignoring implicit relations is a clear limitation of this study that, with the help of more advanced discourse parsers, should be overcome in future research.

Lastly, samples have shown a relatively high proportion of parsing mistakes related to causal relations. In particular, the discourse parser used in this experiment seems to be prone to incorrectly classifying discourse connectives that can have both temporal and causal semantics, such as \textit{since} and \textit{as}. These incorrect annotations weaken the effects of causal relations on helpfulness predictions as well as the overall reliability of the results. Again, to obtain more reliable results, any improvements made in automatic discourse parsing should be taken advantage of in further studies.

% H2

The second hypothesis of this study is at least partly supported by the obtained results.

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H2}] The effect of individual discourse-relation types on predicting review helpfulness differs among different product categories.
\end{enumerate}

The observation that around half of the examined discourse-relation types show similar effects in both investigated product categories while the other half has diverging impacts suggests the existence of mechanisms that are shared within the text type while others being sensitive to product categories. To test this assumption, however, reviews from more than just two product categories will have to be compared.

It should also be noted that \citet{MudambiSchuff2010} base their findings on different evaluations of product reviews between product types on the distinction of search and experience goods. The product categories used in this study, however, do not clearly separate between these types. Even though electronic products typically include many search goods (such as printers) and the book category contains a large number of experience goods (such as fiction books), both categories also comprise a number of cross-classified product groups, such as headphones or non-fiction. A clearer separation of search and experience goods might lead to more pronounced results.


\section{Conclusion}
\label{sec:conclusion}

This thesis investigated the effect of individual discourse-relation types on predicting review helpfulness. Contrary to expectations, the highest predictive power was not found in causal relations, but rather in conjunction relations. Whether this is because the latter are often used to signal additional confirming statements, because their existence in a review is correlated with its writing quality, or because they are indicative of premises supporting a claim will have to be examined in further studies.

As a second finding, the influence of one half of the analyzed discourse-relation types on predicting review helpfulness has been shown to differ among two product categories, while the other half exhibited similar effects across both categories. Future comparisons of reviews clustered into more product categories and types (such as search and experience goods) will show whether this indicates the presence of certain mechanisms being common to the complete text type and others being sensitive to product types.

In total, the study presents a number of novel and partly unexpected findings and and opens up a range of research avenues. However, it also shows that a significant amount of additional research is needed to better understand the influence of discourse structure on review helpfulness.

\vfill


\begin{center}
Both code and data used for this study are freely available for research purposes at \url{https://github.com/s-go/th-review-helpfulness}.
\end{center}

\newpage
\begin{thebibliography}{9}
\interlinepenalty=10000

\bibitem[Almagrabi et al.(2015)]{Almagrabi2015} Almagrabi, H., Malibari, A., \& McNaught, J. (2015). A Survey of Quality Prediction of Product Reviews. In \textit{International Journal of Advanced Computer Science \& Applications}, 1(6), 49-58.

\bibitem[Chen \& Tseng(2011)]{ChenTseng2011} Chen, C. C., \& Tseng, Y. D. (2011). Quality evaluation of product reviews using an information quality framework. In \textit{Decision Support Systems}, 50(4), 755-768.

\bibitem[Eckle-Kohler et al.(2015)]{Eckle-Kohler2015} Eckle-Kohler, J., Kluge, R., \& Gurevych, I. (2015). On the Role of Discourse Markers for Discriminating Claims and Premises in Argumentative Discourse. In \textit{EMNLP}, 2236-2242.

\bibitem[Golly(2017)]{Golly2017} Golly, S. (2017). \textit{Predicting the Helpfulness of Product Reviews Using Discourse Relations.} Unpublished term paper. University of Potsdam, Potsdam, Germany. Retrieved from \url{https://github.com/s-go/cr-review-helpfulness/raw/master/docs/cr-review-helpfulness.pdf}

\bibitem[He and McAuley(2016)]{HeMcAuley2016} He, R., \& McAuley, J. (2016). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In \textit{Proceedings of the 25th International Conference on World Wide Web}, International World Wide Web Conferences Steering Committee, 507-517.

\bibitem[Kamalski et al.(2008)]{Kamalski2008} Kamalski, J., Lentz, L., Sanders, T., \& Zwaan, R. A. (2008). The forewarning effect of coherence markers in persuasive discourse: Evidence from persuasion and processing. In \textit{Discourse Processes}, 45, 545–579.

\bibitem[Kim et al.(2006)]{Kim2006} Kim, S. M., Pantel, P., Chklovski, T., \& Pennacchiotti, M. (2006). Automatically assessing review helpfulness. In \textit{Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing}, Association for Computational Linguistics, 423-430.

\bibitem[Lin et al.(2014)]{Lin2014} Lin, Z., Ng, H. T., \& Kan, M. Y. (2014). A PDTB-styled end-to-end discourse parser. In \textit{Natural Language Engineering}, 20(2), 151-184.

\bibitem[Liu et al.(2008)]{Liu2008} Liu, Y., Huang, X., An, A., \& Yu, X. (2008). Modeling and Predicting the Helpfulness of Online Reviews. In \textit{Data mining}, 2008. ICDM'08. Eighth IEEE international conference on Data mining, 443-452.

\bibitem[Marcus et al.(1993)]{Marcus1993} Marcus, M. P., Marcinkiewicz, M. A., \& Santorini, B. (1993). Building a large annotated corpus of English: The Penn Treebank. In \textit{Computational linguistics}, 19(2), 313-330.

\bibitem[Mertz et al.(2014)]{Mertz2014} Mertz, M., Korfiatis, N., \& Zicari, R. V. (2014). Using Dependency Bigrams and Discourse Connectives for Predicting the Helpfulness of Online Reviews. In \textit{International Conference on Electronic Commerce and Web Technologies}, Springer International Publishing, 146-152.

\bibitem[Mudambi and Schuff(2010)]{MudambiSchuff2010} Mudambi, S. M., \& Schuff, D. (2010). What Makes a Helpful Online Review? A Study of Customer Reviews on Amazon.com. In \textit{MIS Quarterly}, 34(1), 185-200.

\bibitem[Nelson(1970)]{Nelson1970} Nelson, P. (1970). Information and consumer behavior. In \textit{Journal of political economy}, 78(2), 311-329.

\bibitem[Prasad et al.(2008)]{Prasad2008} Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., \& Webber, B. (2008). \textit{The Penn Discourse Treebank 2.0.} Paper presented at the 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakesh, Morocco.

\bibitem[Prasad et al.(2007)]{Prasad2007} Prasad, R., Miltsakaki, E., Dinesh, N., Lee, A., Joshi, A., Robaldo, L., \& Webber, B. L. (2007). \textit{The Penn Discourse Treebank 2.0 Annotation Manual.}

\bibitem[Stab and Gurevych(2014)]{StabGurevych2014} Stab, C., \& Gurevych, I. (2014). Identifying Argumentative Discourse Structures in Persuasive Essays. In \textit{EMNLP}, 46-56.

\bibitem[Siering \& Muntermann(2013)]{SieringMuntermann2013} Siering, M., \& Muntermann, J. (2013). What Drives the Helpfulness of Online Product Reviews? From Stars to Facts and Emotions. In \textit{Wirtschaftsinformatik} (Vol. 7). 103-118.

\bibitem[Webber et al.(2006)]{Webber2006} Webber, B., Joshi, A., Miltsakaki, E., Prasad, R., Dinesh, N., Lee, A., \& Forbes, K. (2006). A Short Introduction to the Penn Discourse TreeBank. In \textit{Copenhagen Studies in Language}, 32(9).

\bibitem[Zhang et al.(2015)]{Zhang2015} Zhang, Z., Ma, Y., Chen, G., \& Wei, Q. (2015, June). Extending associative classifier to detect helpful online reviews with uncertain classes. In \textit{IFSA-EUSFLAT}. 1134-1139.

\bibitem[Zhang \& Varadarajan(2006)]{ZhangVaradarajan2006} Zhang, Z., \& Varadarajan, B. (2006). Utility scoring of product reviews. In \textit{Proceedings of the 15th ACM international conference on Information and knowledge management}. 51-57.

\end{thebibliography}


\pagebreak
\newgeometry{left=40mm, right=40mm}

\vspace*{10em}
\section*{Erklärung}

\begin{sloppypar}
Hiermit versichere ich, dass ich die vorliegende Arbeit selbständig und ohne Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe. Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten und nicht veröffentlichten Quellen entnommen sind, sind als solche kenntlich gemacht. Die Arbeit hat in gleicher oder ähnlicher Form noch keiner anderen Prüfungsbehörde vorgelegen.
\end{sloppypar}

\vspace{1em}

\begin{center}
Berlin, den 16. November 2017

\vspace{2.4em}

Sebastian Golly
\end{center}

\thispagestyle{empty}

\end{document}
