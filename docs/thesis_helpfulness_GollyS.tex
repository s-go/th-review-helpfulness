\documentclass[
    %german,
    a4paper,%
    %11pt,%
    12pt,%
    oneside,%
    %twoside,%
    %titlepage,%
    %liststotoc,%
    toc=bibliography,
    %bibtotoc,%
    %headinclude,%
    %draft,
    final,
    %pointlessnumbers,%
    %fleqn,% mathematische Gleichungen linksbündig statt zentriert
]{scrartcl}

%\usepackage{polyglossia} % (neue) deutsche Beschriftungen und Silbentrennung
%\setdefaultlanguage[spelling=new]{german}
%\usepackage{times} % Nimbus Roman statt CM Serif
\usepackage{lmodern} % Latin Modern (in T1) statt CM
%%\usepackage[T1]{fontenc} % T1-Kodierung: Umlaute als *eine* Glyphe
%%\usepackage[utf8]{inputenc}
\usepackage{fontspec}
\setsansfont{Myriad Pro}
\setmainfont{Adobe Garamond Pro}

%%%%%%%%%%%%%%%% Seitenspiegel, Typografie %%%%%%%%%%%%%%%%%%%
%\usepackage{pdflscape} % Querformat: \begin{landscape}
\usepackage{geometry} % Seitenränder selbst bestimmen
\geometry{a4paper,%
          top=18mm,%
          left=20mm,%
          right=20mm,% ohne Marginalien: 20mm - mit Marginalien: 45mm
          bottom=22mm,%
          headsep=10mm,%
          footskip=12mm,%
         }
\setlength{\parindent}{0pt} % kein Einruecken bei Absatzbeginn
\setlength{\parskip}{8pt} % Absaetze durch Abstand kennzeichnen (1 Zeichenhoehe)

% Space between footnote mark and text
\usepackage[hang]{footmisc}
\setlength{\footnotemargin}{1em}

\usepackage{caption}
\captionsetup{
  labelfont=bf,
}

\usepackage{threeparttable}
\renewcommand{\TPTminimum}{\linewidth}

\usepackage{makecell} % \thead{} \makecell{}
\renewcommand\theadalign{cb}
\renewcommand\theadfont{\bfseries}

\usepackage{ulem} % durchgestrichener Text: \sout{}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor} % schönere Farben, z. B. RawSienna
\usepackage{enumerate} % Aufzählungsstil anpassen, z. B. {enumerate}[a)] – \setcounter{enumi}{4}

\usepackage{natbib} % \bibitem[Guevara(2010)]{Guevara2010} - \citet[5]{Guevara2010}
\bibpunct{(}{)}{;}{a}{}{,} % Interpunktion in Zitaten
\setcitestyle{notesep={: }} % Doppelpunkt zwischen Jahr und Seitenzahl

\usepackage{tocstyle}
\newtocstyle[KOMAlike][leaders]{alldotted}{}
\usetocstyle{alldotted}

%%%%%%%%%%%%%%%% Quelltext-Satz %%%%%%%%%%%%%%%%%%%
\usepackage{textcomp} % Text Companion fonts (für einfache Anführungszeichen)
\usepackage{listings} % Umgebung lstlisting; \lstinline$...$
\lstset{
	language=,                % the language of the code
	basicstyle=\ttfamily\small,
	xleftmargin=2em,
	xrightmargin=2em,
	captionpos=b,
	abovecaptionskip=.5em,
	commentstyle=\color{OliveGreen},% sets comment style
	tabsize=3,                      % sets default tabsize
	breaklines=true,                % sets automatic line breaking
	breakatwhitespace=true,         % sets if automatic breaks should only happen at whitespace
	showspaces=false,               % show spaces adding particular underscores
	showstringspaces=false,         % underline spaces within strings
	escapechar=§,                   % escapes to LaTeX
	columns=flexible,               % columns=fixed / columns=flexible / columns=fullflexible
	upquote=true,                   % straight quotes
	literate={ö}{{\"o}}1            % national characters:
			 {ä}{{\"a}}1            %   *{replace}{replacement text}{length in output}
			 {ü}{{\"u}}1            %   * (optional): not in delimited text (strings, comments, ...) 
			 {Ö}{{\"O}}1
			 {Ä}{{\"A}}1
			 {Ü}{{\"U}}1
			 {ß}{{\ss}}2
	}

%%%%%%%%%%%%%%%%%%% Linguistik-Pakete %%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{mathtools} % includes amsmath, adds some nice fixes
\usepackage{amssymb} % für \varnothing
%\usepackage{semantic} % für |[ |]
%\usepackage{qtree} % qtree \Tree [. ] [. ] \qroof{}.
%\usepackage[x11names, rgb]{xcolor} % für dot2tex
%\usepackage{tikz} % für dot2tex
%\usetikzlibrary{arrows,shapes} % für dot2tex
         %%% unbedingt nach tikz-Paketen laden %%%
\usepackage{gb4e} % Beispiel-Umgebung: \begin{exe} \ex \begin{xlist}
%\usepackage{avm}             % für AVMs
%\avmfont{\sc}                % allgemeine AVM-Schriftart
%\avmvalfont{\it}             % Schriftart für Werte
%\avmsortfont{\footnotesize\it} % Schriftart für Sort Labels 

%%%%%%%%%%%%%%%%%%% hyperref %%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[hidelinks]{hyperref} % letzter Paketaufruf!
\makeatletter % changes the catcode of @ to 11
\AtBeginDocument{
  \hypersetup{ % hyperref: \title und \author in PDF-Eingenschaften übernehmen
    pdftitle = {\@title},
    pdfsubject = {\@subject},
    pdfauthor = {\@author}
  }
}
\makeatother % changes the catcode of @ back to 12

%%%%%%%%%%%%%%%%%%%%%%% Titel %%%%%%%%%%%%%%%%%%%%%%%%%%
\subject{Bachelor Thesis}
\title{Investigating the \\Impact of Discourse Relations\\ on Review Helpfulness}
\subtitle{}
%\setkomafont{author}{\normalsize}
\setkomafont{date}{\normalsize}
\setkomafont{publishers}{\normalsize}
\author{Sebastian Golly\\ {\normalsize (761737)}}
\date{\today}

% Double line spacing for text body, but not for titles
\usepackage[onehalfspacing]{setspace}
\addtokomafont{disposition}{\linespread{1}}

% No extra line spacing between list items
\usepackage{enumitem}
\setlist{nosep}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\vfill

\paragraph{Abstract}
This study investigates what influence individual discourse-relation types have on predicting the helpfulness of product reviews, and whether these effects vary across product categories. To this end, two probabilistic models are trained on electronics and book reviews, using the distribution of discourse-relation types as their only features. Based on feature-weight analyses and comparisons across the product categories, two key results are drawn: a) Other than expected, not causal, but conjunction relations contribute most to predicting review helpfulness. b) When comparing the results of both product categories, around half of the relation types exhibit similar effects on review helpfulness, while the other half shows differing patterns.
\\[3em]

\vfill

\begin{center}
University of Potsdam\\[1.5em]
Department of Linguistics
\end{center}

\thispagestyle{empty}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}

The task of automatically assessing the helpfulness of product reviews has received considerable attention over the last ten years (cf. \citealt{Almagrabi2015} for a survey). \citet[186]{MudambiSchuff2010} define a helpful review as “a peer-generated product evaluation that facilitates the consumer’s purchase decision process.”

While the helpfulness of reviews is currently largely evaluated manually by users who are asked for their votes by answering a question like “Was this review helpful to you?” it makes sense to find automatic solutions for assessing review helpfulness. These solutions will enable website operators to instantly obtain a helpfulness assessment for every single review, with less interference by human biases, and, like that, reliably rank reviews according to their helpfulness.

Various kinds of features have been proposed for training probabilistic models to predict review helpfulness \citep[cf.][50ff.]{Almagrabi2015}. However, taking into account discourse structure as an indicator for this task is a rather novel approach first explored by \citet{Mertz2014}. Recently, \citet{Golly2017} provides some evidence for the general claim that discourse structure influences the helpfulness of product reviews.

The goal of this study is to obtain a better understanding of how individual discourse-relation types affect review helpfulness. In addition, I want to investigate whether these effects vary across different product categories. To do so, two separate probabilistic models are trained on electronics and book reviews, using the distribution of discourse-relation types as their only features. Insights are drawn based on analyses of their feature weights and comparisons across product categories.

This study is structured as follows: Section~\ref{sec:related-work} gives an overview of relevant previous work. Section~\ref{sec:goal} presents the research objectives and section~\ref{sec:method} the methods applied for investigating them. The experimental results are described in section~\ref{sec:results} and interpreted in section~\ref{sec:discussion}. Finally, section~\ref{sec:conclusion} briefly summarizes and concludes the study.


\section{Related Work}
\label{sec:related-work}

\citet[186]{MudambiSchuff2010} define review helpfulness as “a measure of perceived value in the decision-making process” of a consumer. They show that product type has an influence on how the helpfulness of reviews is evaluated. Basing on a classification introduced by \citet{Nelson1970}, they distinguish between \textit{search goods} whose relevant properties can easily be compared without necessarily having to interact with them (e.g., laser printers, computer screens, or tires) and \textit{experience goods} whose key attributes are subjective and thus difficult to evaluate prior to interaction (e.g., music CDs, headphones, or wine).

% Kim
There have been a number of studies working towards the goal of automatically assessing the helpfulness of product reviews (cf. \citealt{Almagrabi2015}). A very influential one is that by \citet{Kim2006} who propose a Support Vector Machine (SVM) regression model for predicting review helpfulness. They systematically investigate how well different types of features capture the helpfulness of a review and evaluate different combinations of these features.

% Mertz
\citet{Mertz2014} build upon the model proposed by \citet{Kim2006}, evaluating the use of discourse connectives as additional features. In their experiment, however, this does not yield a significant improvement in predicting review helpfulness. This can be explained by several restrictions in their experimental setup.

% PDTB (RST)
Among the most widely used frameworks for modeling discourse structure is PDTB (Penn Discourse Treebank 2.0), as presented in \citet{Prasad2008}. PDTB provides a lexically grounded approach, that is, relations are usually signaled by a discourse marker. In case a relation holds between discourse units that is not explicitly signaled, an implicit discourse marker is annotated, if possible.

As discourse markers are often ambiguous in meaning (\textit{since}, for example, can have both temporal and causal semantics), PDTB provides an annotation of discourse-relation types, so-called \textit{senses}, used for disambiguation. These sense tags are structured in a three-level hierarchy where nested tags inherit the properties of their parents \citep[cf.][5]{Prasad2008}. The subtypes \lstinline|Reason| and \lstinline|Result|, for instance, both inherit from the type \lstinline|Cause|, which, in turn, is a child of the \lstinline|Contingency| class.

Separate relation types have been defined for \textit{pragmatic} uses of connectives, e.g., while the \lstinline|Cause| relation is used to relate two facts in the world, its \lstinline|Pragmatic Cause| counterpart is chosen when a discourse segment provides justification for a claim made in the discourse \citep[cf.][29]{Prasad2007}.

% Lin
With their \textit{PDTB-styled end-to-end discourse parser}, \citet{Lin2014} present a tool for automatically annotating text with PDTB sense tags. It achieves high performance in classifying explicitly signaled relations, while suffering from poor performance with regard to implict relations.

% Golly
Using the PDTB framework and the discourse parser by \citet{Lin2014}, \citet{Golly2017} shows that the presence of certain discourse-relation types in product reviews has, in fact, an effect on their perceived helpfulness to other users. As this experiment employs an SVM regression model with a non-linear RBF kernel that does not allow for feature-weight analysis, the contributions of individual discourse-relation types on helpfulness predictions could not be investigated.

\section{Goal of this Study}
\label{sec:goal}

\citet{Golly2017} has shown that there is an effect of the distribution of discourse relations in product reviews on their perceived helpfulness to other users. It is still unclear, however, how certain discourse-relation types affect review helpfulness. The present study aims to be a first step towards filling this research gap by investigating two questions:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q1}] What is the effect of individual discourse-relation types on predicting review helpfulness?
\end{enumerate}

I expect product reviews that do not only make claims, but justify and reason these claims to be more comprehensible and credible to other users, offering an increased value in their purchase-decision process, and, consequently, being perceived as more helpful.

\textbf{Example:}
\vspace{-1em}

\begin{exe}
\ex Personal video glasses are a difficult item to purchase \textit{(Claim)}
\ex Personal video glasses are a difficult item to purchase because it’s very hard to find a store that carries them \textit{(Justified claim)}
\end{exe}
\vspace{-.5em}

This expectation can be phrased as a first hypothesis:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H1}] Causal relations (\lstinline|Contingency.Cause| and \lstinline|Contingency.Pragmatic Cause|, in terms of PDTB sense tags) have a particularly high impact on review helpfulness.
\end{enumerate}

The second research question considers potential distinctions between product categories. \citet{MudambiSchuff2010} find that reviews on different product types are evaluated differently with respect to their helpfulness. They report both the effects of star ratings and review lengths to vary between search and experience goods. Basing on this observation, I want to investigate whether the effects of discourse structure vary across product categories as well, e.g., when comparing electronic products to books.

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q2}] Does the impact of discourse-relation types on review helpfulness vary across product categories?
\end{enumerate}

As reviews of search goods tend to focus on “objective claims about tangible attributes” \citep[189]{MudambiSchuff2010} whereas reviews of experience goods generally include more “subjective evaluations” (ibid.) and consumers seem to assess review helpfulness differently for these product types, I assume this dissimilarity to reflect in the effect of discourse relations, as well. This leads to the second hypothesis of this study:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H2}] The effect of individual discourse-relation types on predicting review helpfulness differs among different product categories.
\end{enumerate}

\section{Method}
\label{sec:method}

For investigating the research questions presented in the previous section, two probabilistic models have been trained on reviews from different product categories. This section descibes the datasets used in the experiments, the learning task and features of the probabilistic models, as well as the overall experimental setup.

\subsection{Data}

From the 5-core corpus of \textit{Amazon.com} reviews created by \citet{HeMcAuley2016},\footnote{Available at \url{http://jmcauley.ucsd.edu/data/amazon/}.} two product categories have been chosen:

\begin{itemize}
\item “Electronics” (including many search goods) and
\item “Books” (including many experience goods).
\end{itemize}

Each of the reviews in the two datasets includes, among others:

\begin{itemize}
\item the text of the review,
\item its overall product rating (1 to 5 stars),
\item the numbers of positive and total helpfulness votes it has received.
\end{itemize}

Listing~\ref{sample-review} shows a shortened sample review from the corpus.

\begin{lstlisting}[basicstyle=\ttfamily\small\singlespacing, caption=Sample review (shortened) from the “Electronics” category., label=sample-review, float]
{
    "asin": "B000W9OJVA",
    "helpful": [
        103,
        105
    ],
    "overall": 4.0,
    "reviewText": "I recently purchased a set of these. Personal video glasses are a difficult item to purchase because it's very hard to find a store that carries them, let alone lets you try them on before purchase, so I wasn't quite sure what to expect. [...]",
    "reviewTime": "03 27, 2008",
    "reviewerID": "AVPNQUVZWMDSX",
    "reviewerName": "esanta \"esanta\"",
    "summary": "Fantastic!",
    "unixReviewTime": 1206576000
}
\end{lstlisting}

The review texts have been preprocessed to facilitate sentence-boundary detection, as many reviews missed whitespace after sentence-final punctuation or showed a use of ellipses instead of periods.

In order to use only informative items, reviews with less than ten helpfulness votes or with a text shorter than 20 characters have been filtered out. For each of the two product categories, 20,000 reviews have been randomly sampled and partitioned into a development dataset containing 2,000 reviews and a cross-validation dataset containing 18,000 reviews.\footnote{Available at \url{https://drive.google.com/open?id=0B4FHGozCmQFEeTQtMUEzLWpLcTA}.}

\subsection{Learning Task}

In this study, the task of predicting the helpfulness of product reviews is seen as a regression problem. Given a product review and its metadata, the probabilistic model predicts a score that captures the helpfulness of the review.

For learning the model, the helpfulness score of a product review can be simply defined as the ratio of its positive helpfulness votes to its total helpfulness votes. \citet[424]{Kim2006} formalize this metric as the helpfulness function $ h $ that computes the helpfulness score of a review $ r $:

\vspace{-2em}

\begin{align}
\label{helpfulness-fuction}
h(r \in R) = \frac{rating_+(r)}{rating_+(r) + rating_-(r)}
\end{align}

This helpfulness ratio is defined for every review with at least one helpfulness vote. It ranges between 0 (minimum helpfulness) and 1 (maximum helpfulness). For training and evaluation, the gold standard of helpfulness scores is determined based on the helpfulness votes given by \textit{Amazon.com} users that are indicated in the corpus.

\subsection{Features}

In order to answer the research questions, the review texts had to be annotated with discourse relations. PDTB sense tags have been extracted automatically using the \textit{PDTB-styled end-to-end discourse parser} \citep{Lin2014}. It can be configured to extract sense tags at any of the three levels of the PDTB sense hierarchy \citep[cf.][5]{Prasad2008}. As a reasonable compromise between accuracy and parsing performance, sense tags from the second (\textit{type}) level have been extracted (e.g., \lstinline|Comparison.Contrast| or \lstinline|Contingency.Cause|). Because of the unsatisfactory performance of the non-explicit classifier \citep[cf.][175]{Lin2014}, only explicitly signalled relations have been used in the experiments.

% TODO: Why PDTB? Why this parser?

\begin{sloppypar}
Spot tests showed that the discourse parser was unable to correctly distinguish pragmatic PDTB types (\lstinline|Contingency.Pragmatic cause|, \lstinline|Contingency.Pragmatic condition|, \lstinline|Comparison.Pragmatic contrast|, and \lstinline|Comparison.Pragmatic concession|) from their non-pragmatic counterparts. As this distinction is not crucial to the intended analyses, I decided to merge the pragmatic types with their non-pragmatic variants. That is, the \lstinline|Contingency.Cause| feature also includes all instances of \lstinline|Contingency.Pragmatic cause| relations, covering both purely causal and justifying relations.
\end{sloppypar}

(\ref{ex:pdtb-types-first}–\ref{ex:pdtb-types-last}) show typical examples for each of the extracted PDTB types, with the respective connective typeset in bold.

\begin{exe}
\ex\label{ex:pdtb-types-first} \lstinline|Temporal.Asynchronous|\\
The internal cables could be a tiny bit longer, so measure carefully inside your case \textbf{before} you purchase

\ex \lstinline|Temporal.Synchrony|\\
the DVD sounded like it was going through a meat grinder \textbf{when} it was playing

\ex \lstinline|Contingency.Cause|\\
i got this lens \textbf{because} i wanted a compact wide angle lens that i could easily throw in my bag to cover events.

\ex \lstinline|Contingency.Condition|\\
So \textbf{if} you want a great 70-200 zoom and crave attention and can carry the weight this lens is for you.

\ex \lstinline|Comparison.Contrast|\\
Yeah, you'll pay a little more, \textbf{but} the quality you'll get will be worth it.

\ex \lstinline|Comparison.Concession|\\
\textbf{Although} the camera cannot zoom very far it still is a very good camera.

\ex \lstinline|Expansion.Conjunction|\\
The kit lenses are a great starter. \textbf{And} they're worlds better than anything you'll get with a point-and-shoot.

\ex \lstinline|Expansion.Instantiation|\\
The only problem i have with the camera is that it has a hard time focusing on small texts. \textbf{For example} i want to copy a receipt and send it to the manufacturer it would come out blurry.

\ex \lstinline|Expansion.Restatement|\\
\textbf{Overall}, I am happy with the build and performance of these chargers.

\ex \lstinline|Expansion.Alternative|\\
you need to press it slow and firm, \textbf{or} it won't start.

\ex \lstinline|Expansion.Exception|\\
wifi works well, \textbf{except} that streaming does not work all the time.

\ex\label{ex:pdtb-types-last} \lstinline|Expansion.List|\\
The keyboard is great, screen is good \textbf{and} overall quality is very good.
\end{exe}

Based on the extracted occurrences of discourse relations in each review, three variants of discourse-relation features have been aggregated:

\begin{itemize}
\item \lstinline|REL-CNT|: occurrence frequencies of explicit discourse-relation types in each review (listing \ref{feat-rel-cnt} shows an instance of this feature for a sample review),
\item \lstinline|REL-IPT|: normalized frequencies of explicit discourse-relations types (as instances per token),
\item \lstinline|REL-PRS|: presence of explicit discourse-relation types in each review (with values \lstinline|1| or \lstinline|0| according to whether or not a certain discourse-relation type occurs in the review).
\end{itemize}

\begin{lstlisting}[caption=\texttt{REL-CNT} feature for a sample review., label=feat-rel-cnt, basicstyle=\ttfamily\small\singlespacing]
  {
      'Comparison.Concession': 0,
      'Comparison.Contrast': 13,
      'Comparison.Pragmatic concession': 0,
      'Comparison.Pragmatic contrast': 0,
      'Contingency.Cause': 8,
      'Contingency.Condition': 2,
      'Contingency.Pragmatic condition': 0,
      'Expansion.Alternative': 0,
      'Expansion.Conjunction': 17,
      'Expansion.Exception': 0,
      'Expansion.Instantiation': 0,
      'Expansion.List': 0,
      'Expansion.Restatement': 0,
      'Temporal.Asynchronous': 8,
      'Temporal.Synchrony': 6
  }
\end{lstlisting}

For training and evaluating the models, each feature was scaled to the range $ [-1, 1] $.


\subsection{Experimental Setup}

Similar to \citet{Kim2006} and \citet{Golly2017}, a Support Vector Machine (SVM) regression model has been trained to predict review helpfulness. However, as the feature weights of the non-linear RBF kernel that has been used in previous studies cannot be analyzed meaningfully, a linear kernel has been chosen instead. Pre-studies showed that, with thorough tuning of the hyperparameters $ C $ and $ \epsilon $ (performing a full-grid search on the development dataset), an equal level of prediction performance can be reached even with the less powerful linear kernel.

In contrast to \citet{Golly2017} that used discourse-relation features in addition to a set of baseline features (overall rating, review length, and unigram statistics), the models in this study rely only on discourse-relation features in order to eliminate cross-effects with other factors.

For the purpose of comparing feature weights between product categories, separate models have been trained on reviews from the two datasets (“Electronics” and “Books”).

For evaluating the results, ten-fold cross-validation has been applied, where each model was trained using nine folds, and its performance was evaluated on the remaining test fold. As an evaluation metric, the Pearson correlation coefficient (\textit{Pearson's r}) between the predicted helpfulness scores and the gold standard (based on the \textit{Amazon.com} helpfulness votes) has been computed. It ranges between -1 (total negative correlation) and +1 (total positive correlation).

Table~\ref{tab:disc-features-performance} compares the performance of the three discourse-feature variants presented in the previous subsection. In either of the two product categories, \lstinline|REL-PRS| that captures the presence of each discourse-relation type in a review yields the best performance. For this reason, it was selected for training and evaluating the models.

\begin{table}[h!]
	\centering
	
	\caption{Evaluation results of a linear SVR model using different variants of discourse-relation features}
	\label{tab:disc-features-performance}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{Feature} & \thead{Electronics\\ Pearson's \textit{r}\tnote{a}} & \thead{Books\\ Pearson's \textit{r}\tnote{a}} \\ \hline
		\lstinline|REL-CNT| & 0.211 (± 0.040) & 0.167 (± 0.058) \\ \hline
		\lstinline|REL-IPT| & 0.086 (± 0.040) & 0.125 (± 0.064) \\ \hline
		\lstinline|REL-PRS| & 0.279 (± 0.056) & 0.237 (± 0.084) \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
	\centering
	\footnotesize
	\item[a] 95\% confidence bounds are calculated using 10-fold cross-validation.
	\end{tablenotes}
	
	\end{threeparttable}

\end{table}

\section{Results}
\label{sec:results}

This section presents the results with regard to both research questions. They show that the discourse-relation type that contributes most to predicting review helpfulness is \lstinline|Expansion.Conjunction|, with causal relations having substantially lower impact. When comparing the results between both product categories, around half of the relation types exhibit similar impact on review helpfulness, while the other half shows differing effects.

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q1}] What is the effect of individual discourse-relation types on predicting review helpfulness?
\end{enumerate}

Tables~\ref{tab:feature-weights-electronics} and \ref{tab:feature-weights-books} rank the features of both probabilistic models (for electronics and book reviews) according to their learned regression coefficients, or feature weights.
The coefficients represent the mean change in the predicted helpfulness score if the corresponding discourse-relation type is present in a review (while holding the other predictors in the model constant). 

For both product categories, the presence of explicit \lstinline|Expansion.Conjunction| relations proves to be by far most predictive of review helpfulness. This discourse-relation type is typically expressed by connectives such as \textit{and} or \textit{also}.

\begin{table}[h!]
	\centering
	
	\caption{Ranking of discourse-relation types according to their feature-weight coefficients in the model trained on electronics reviews}
	\label{tab:feature-weights-electronics}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{clr}
		\textbf{Rank} & \textbf{PDTB Sense Tag} & \textbf{Coefficient}\tnote{\textit{a}} \\ \hline
		1 & Expansion.Conjunction & 0.048 \\ \hline
		2 & Temporal.Synchrony  & 0.023 \\ \hline
		3 & Comparison.Contrast\tnote{\textit{b}} & 0.018 \\ \hline
		4 & Contingency.Condition\tnote{\textit{b}}  & 0.012 \\ \hline
		5 & Contingency.Cause\tnote{\textit{b}}  & 0.011 \\ \hline
		6 & Comparison.Concession\tnote{\textit{b}} & 0.009 \\ \hline
		7 & Expansion.Restatement  & 0.008 \\ \hline
		8 & Expansion.List  & 0.004 \\ \hline
		9 & Temporal.Asynchronous    & 0.001 \\ \hline
		10 & Expansion.Instantiation & -0.000 \\ \hline
		11 & Expansion.Exception  & -0.001 \\ \hline
		12 & Expansion.Alternative  & -0.003 \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
	\centering
	\footnotesize
	\item[\textit{a}] Coefficients calculated using 10-fold cross-validation.
	\item[\textit{b}] Including \textit{Pragmatic} type.
	\end{tablenotes}
	
	\end{threeparttable}
\end{table}

\begin{table}[h!]
	\centering
	
	\caption{Ranking of discourse-relation types according to their feature-weight coefficients in the model trained on book reviews}
	\label{tab:feature-weights-books}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{clr}
		\textbf{Rank} & \textbf{PDTB Sense Tag} & \textbf{Coefficient}\tnote{\textit{a}} \\ \hline
		1 & Expansion.Conjunction & 0.093 \\ \hline
		2 & Temporal.Synchrony  & 0.028 \\ \hline
		3 & Comparison.Contrast\tnote{\textit{b}} & 0.017 \\ \hline
		4 & Expansion.Instantiation & 0.015 \\ \hline
		5 & Temporal.Asynchronous    & 0.013 \\ \hline
		6 & Comparison.Concession\tnote{\textit{b}} & 0.007 \\ \hline
		7 & Expansion.Restatement  & 0.000 \\ \hline
		8 & Contingency.Condition\tnote{\textit{b}}  & -0.006 \\ \hline
		9 & Contingency.Cause\tnote{\textit{b}}  & -0.006 \\ \hline
		10 & Expansion.Alternative  & -0.025 \\ \hline
		11 & Expansion.Exception  & -0.033 \\ \hline
		12 & Expansion.List  & -0.041 \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
	\centering
	\footnotesize
	\item[\textit{a}] Coefficients calculated using 10-fold cross-validation.
	\item[\textit{b}] Including \textit{Pragmatic} type.
	\end{tablenotes}
	
	\end{threeparttable}
\end{table}

As this result is rather unexpected, it deserves a more detailed investigation. (\ref{ex:conjunction-first}–\ref{ex:conjunction-last}) show typical examples for instances of \lstinline|Expansion.Conjunction| relations in the corpus. The connectives signaling the relation are typeset in bold. It can be observed that, in many of the examples, the conjunction relation is used to present an additional argument, often for buying the reviewed product.

\begin{exe}
\ex \label{ex:conjunction-first} The kit lenses are a great starter. \textbf{And} they're worlds better than anything you'll get with a point-and-shoot.
\ex Plenty of reviews here on how great the camera is, \textbf{and} I agree with them.
\ex You're not just buying an e500 here, you're \textbf{also} buying into Olympus's lens system.
\ex The image quality is on par with similar offerings from Nikon and Canon, \textbf{and} it has the same, or very similar features.
\ex Spend a little more now, \textbf{and} it'll literally save you about \$200 later.
\ex The 14-54 is built like a tank \textbf{and} the image quality will blow you away.
\ex \label{ex:conjunction-last} Overall, the sets are very similar, my personal choice being the Haier, as I think the picture is sharper, the price is better and the battery lasts longer for my needs in bad weather. Haier \textbf{also} has way more picture settings and adjustments in the tv's menu
\end{exe}

When analyzing samples of reviews without any instance of an \lstinline|Expansion.Conjunction| relation, they often exhibit one or more of the following properties:

\begin{itemize}
\item short overall length,
\item short sentence length,
\item low usage of cohesive devices.
\end{itemize}

(\ref{ex:no-conjunction}) gives a complete example of one of the reviews showing all of these properties.

\begin{exe}
\ex \label{ex:no-conjunction} It works fine.  It was easy to install.  I think that may have something to do with my experience with other routers though.  I am new to this.
\end{exe}

In order to find out whether these indications gained from individual samples generalize over the complete corpus, I analyzed the following features and compared their average values between reviews including and reviews without \lstinline|Expansion.Conjunction| relations:

\begin{itemize}
\item review length: overall number of tokens,
\item sentence length: average number of tokens per sentence,
\item use of connectives: instances of explicit discourse relations per thousand tokens,
\item helpfulness score.
\end{itemize}

Tables~\ref{tab:conjunction-features-electronics} and \ref{tab:conjunction-features-books} show that, for both product categories, reviews including at least one instance of an explicit \lstinline|Expansion.Conjunction| relation have, in average, both higher total and sentence lengths, higher normalized frequencies of explicitly signaled discourse relations, as well as higher helpfulness ratings.


\begin{table}[h!]
	\centering
	
	\caption{Comparison of selected linguistic features between reviews including and reviews without \lstinline|Expansion.Conjunction| relations in electronics reviews}
	\label{tab:conjunction-features-electronics}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{} & \thead{Reviews including\\ Expansion.Conjunction} & \thead{Reviews without\\ Expansion.Conjunction} \\ \hline
		Avg. review length & 364.33 & 101.84 \\ \hline
		Avg. sentence length & 17.64 & 14.95 \\ \hline
		Avg. use of connectives & 38.34 & 23.36 \\ \hline
		Avg. helpfulness score & 0.83 & 0.71 \\ \hline
		\end{tabular} 
	}
	
	\end{threeparttable}

\end{table}


\begin{table}[h!]
	\centering
	
	\caption{Comparison of selected linguistic features between reviews including and reviews without \lstinline|Expansion.Conjunction| relations in book reviews}
	\label{tab:conjunction-features-books}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{} & \thead{Reviews including\\ Expansion.Conjunction} & \thead{Reviews without\\ Expansion.Conjunction} \\ \hline
		Avg. review length & 321.61 & 108.77 \\ \hline
		Avg. sentence length & 19.9 & 16.53 \\ \hline
		Avg. use of connectives & 31.5 & 19.27 \\ \hline
		Avg. helpfulness score & 0.77 & 0.65 \\ \hline
		\end{tabular} 
	}
	
	\end{threeparttable}

\end{table}

Other than expected in the first hypothesis, the presence of \lstinline|Contingency.Cause| relations does not show to be a particularly good predictor of review helpfulness. In electronics reviews, the corresponding feature obtains the fifth rank according to its feature weight. In the model trained on book reviews, it even gets assigned a slightly negative coefficient and only reaches rank nine.

When analyzing samples of the automatically extracted relations, a noticeable number of parsing mistakes with respect to \lstinline|Contingency.Cause| can be observed. (\ref{ex:parsing-mistakes-first}–\ref{ex:parsing-mistakes-last}) present typical examples of such incorrect annotations that seem to occur frequently.

\begin{exe}
\ex \label{ex:parsing-mistakes-first} Annotation of \lstinline|Temporal.Asynchronous| instead of \lstinline|Contingency.Cause|\\
Don't know if the problem is router related or n-version related but \textbf{since} range and not speed is the issue for me, I do not find the unit to be an improvement over what I have already installed.
\ex Annotation of \lstinline|Temporal.Synchrony| instead of \lstinline|Contingency.Cause|\\
The picture is absolutely gorgeous, \textbf{as} it has the new atsc digital signal receiver built in!
\ex \label{ex:parsing-mistakes-last} Annotation of \lstinline|Contingency.Cause| instead of \lstinline|Expansion.Conjunction|\\
I've had time to thoroughly test it \textbf{and} I've sort of begun to have second thoughts about these.
\end{exe}

In sum, the results of the feature-weight analyses do not support the first hypothesis that expected a strong impact of \lstinline|Contingency.Cause| relations on predicting review helpfulness. The analyses of corpus samples and linguistic features, however, provide a number of findings that will be referred to in the discussion section to account for the observed results.

%%% Q2

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{Q2}] Does the impact of discourse-relation types on review helpfulness vary across product categories?
\end{enumerate}

Table~\ref{tab:rank-comparison} compares the ranks of discourse-relation types according to their respective feature weights between the two product categories. Substantial differences are highlighted in bold.

\begin{table}[h!]
	\centering
	
	\caption{Comparison of discourse-relation ranks between models trained on electronics vs. book reviews}
	\label{tab:rank-comparison}
	
	\begin{threeparttable}
	\renewcommand{\arraystretch}{1.5}
	\makebox[\linewidth]{
		\begin{tabular}{lcc}
		\thead{PDTB Sense Tag} & \thead{Electronics Rank\\ (Coefficient)} & \thead{Books Rank\\ (Coefficient)} \\ \hline
		Expansion.Conjunction & 1 (0.048) & 1 (0.093) \\ \hline
		Temporal.Synchrony  & 2 (0.023) & 2 (0.028) \\ \hline
		Comparison.Contrast\tnote{\textit{a}} & 3 (0.018) & 3 (0.017) \\ \hline
		Contingency.Condition\tnote{\textit{a}}  & \textbf{4 (0.012)} & \textbf{8 (-0.006)} \\ \hline
		Contingency.Cause\tnote{\textit{a}}  & \textbf{5 (0.011)} & \textbf{9 (-0.006)} \\ \hline
		Comparison.Concession\tnote{\textit{a}} & 6 (0.009) & 6 (0.007) \\ \hline
		Expansion.Restatement  & 7 (0.008) & 7 (0.000) \\ \hline
		Expansion.List  & \textbf{8 (0.004)} & \textbf{12 (-0.041)} \\ \hline
		Temporal.Asynchronous    & \textbf{9 (0.001)} & \textbf{5 (0.013)} \\ \hline
		Expansion.Instantiation & \textbf{10 (-0.000)} & \textbf{4 (0.015)} \\ \hline
		Expansion.Exception  & 11 \textbf{(-0.001)} & 11 \textbf{(-0.033)} \\ \hline
		Expansion.Alternative  & \textbf{12 (-0.003)} & \textbf{10 (-0.025)} \\ \hline
		\end{tabular} 
	}
	
	\begin{tablenotes}
		\centering
		\footnotesize
		\item[\textit{a}] Including \textit{Pragmatic} type.
	\end{tablenotes}
	
	\end{threeparttable}
\end{table}

Half of the examined discourse-relation types exhibit similar results in both product categories. This includes the three relation types with the strongest positive coefficients that have identical ranks for both electronic products and books. For the remaining relation types, a number of differences can be observed: 

\begin{enumerate}
\item Both types of the \lstinline|Contingency| class (\lstinline|Condition| and \lstinline|Cause|) are positive predictors for electronics reviews, while having small negative coefficients in the model trained on book reviews.
\item \lstinline|Temporal.Asynchronous| and \lstinline|Expansion.Instantiation| relations have clearly positive coefficients in the books domain, while showing no predictive power for electronics reviews.
\item The three \lstinline|Expansion| types \lstinline|List|, \lstinline|Exception|, and \lstinline|Alternative| exhibit a clear negative correlation with the helpfulness of book reviews while having almost no effect in the electronics domain.
\end{enumerate}

With regard to the second hypothesis, the results give a split answer. While one half of the discourse-relation types shows a similar impact on predicting review helpfulness, the other half exhibits differing effects among the two examined product categories.

\section{Discussion}
\label{sec:discussion}

The experimental results do not support the expecation stated in the first hypothesis:

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H1}] Causal relations (\lstinline|Contingency.Cause| and \lstinline|Contingency.Pragmatic Cause|, in terms of PDTB sense tags) have a particularly high impact on review helpfulness.
\end{enumerate}

\begin{sloppypar}
Instead, the relation that has shown to have the highest impact on review helpfulness is \lstinline|Expansion.Conjunction|. There are different potential accounts for this unexpected and even counter-intuitive result.
\end{sloppypar}

One of them can be dubbed \textit{multi-confirmation} account. As “[c]onsumers follow a purchase decision process that seeks to reduce uncertainty” \citep[187]{MudambiSchuff2010}, they read product reviews, at least partly, as a source of confirmation. Once they are interested enough in a product to read its reviews, many of them only seek for an endorsement to go the last step and purchase it. The more confirming statements a review contains, the more helpful it will be perceived by these users. An examination of samples of \lstinline|Expansion.Conjunction| relations in the corpus showed that the relation is often used to present an additional argument for buying the reviewed product. Following the multi-confirmation interpretation, these uses of conjunction relations increase the helpfulness of the corresponding review.

A second account refers to the \textit{writing quality} of the examined reviews. \citet{Liu2008} find that writing style has an influence on review-helpfulness predictions. The analyses performed in this study show that the presence of \lstinline|Expansion.Conjunction| relations is positively correlated with linguistic features such as review length, average sentence length, and density of discourse markers. The observed higher average values of these features might indicate an increased writing quality that results in a gain of perceived review helpfulness.

A third and last account considers findings from argumentation-structure research. \citet[2239]{Eckle-Kohler2015} identify the german discourse connective \textit{und} (\textit{and}) that signals \lstinline|Expansion.Conjunction| relations as being indicative of \textit{premises}. In the claim-premise model of argumentation they adopt, premises are used for supporting (or attacking) a claim. This is in line with the initial expectation that lead to the first hypothesis: the assumption that justified claims (supported by one or more premises) are more comprehensible and credible to other users and thereby increase review helpfulness.

% Accounts for medium/low predictiveness of causal relations

Besides the high predictiveness of \lstinline|Expansion.Conjunction| relations, the observed medium to low predictiveness of causal relations deserves a distinct interpretation. Again, the results can be accounted for from different perspectives.

As mentioned before, the assumption that a high ratio of justified claims gives rise to an increase in helpfulness might be better captured by analyses of argumentation structure rather than discourse structure. In fact, \citet[54]{StabGurevych2014} point out that “approaches relying on discourse markers are not applicable for identifying argumentative discourse structures in documents which do not follow a standardized form.” Thus, it could be promising to investigate this assumption based on an argumentation-mining approach.

Second, it has to be stressed that for the analyses performed in this study, only explicit discourse relations have been taken into account because of the poor performance of current discourse parsers in automatically detecting implicit relations. The expectations that underlie the first hypothesis focus on \textit{subjective causality} (supporting a claim rather than relating two facts). If we classify product reviews as a rather persuasive than informative text type, the findings of \citet{Kamalski2008} provide another explanation to the low impact of causal relations. They find that, in a persuasive context, explicitly marked subjective relations trigger resistance in the readers and thereby reduce the persuasuasive power of the text. In any case, ignoring implicit relations is a clear limitation of this study that, with the help of more advanced discourse parsers, should be overcome in future research.

Lastly, samples have shown a relatively high proportion of parsing mistakes related to causal relations. In particular, the discourse parser used in this experiment seems to be prone to incorrectly classifying discourse markers that can have both temporal and causal meanings, such as \textit{since} and \textit{as}. These incorrect annotations weaken the effects of causal relations on helpfulness predictions as well as the overall reliability of the results. Again, to obtain more reliable results, any improvements made in automatic discourse parsing should be taken advantage of in further studies.

% H2

The second hypothesis of this study is at least partly supported by the obtained results.

\begin{enumerate}[rightmargin=1cm]
\item[\textbf{H2}] The effect of individual discourse-relation types on predicting review helpfulness differs among different product categories.
\end{enumerate}

The observation that around half of the examined discourse-relation types show similar effects in both investigated product categories while the other half has diverging impacts suggests the existence of mechanisms that are shared within the text type while others being sensitive to product categories. To test this assumption, however, reviews from more than just two product categories will have to be compared.

It should also be noted that \citet{MudambiSchuff2010} base their findings on different evaluations of product reviews between product types on the distinction of search and experience goods. The product categories used in this study, however, do not clearly separate between search and experience goods. Even though electronic products typically include many search goods (such as printers) and the book category contains a large number of experience goods (such as fictional literature), both categories also comprise a number of cross-classified product groups, such as headphones or non-fiction. A clearer separation of search and experience goods might lead to more pronounced results.


\section{Conclusion}
\label{sec:conclusion}

This study investigated the effect of individual discourse-relation types on predicting review helpfulness. Contrary to expectations, the highest predictive power was not found in causal relations, but rather in conjunction relations. Whether this is because the latter are often used to signal additional confirming statements, because their existence in a review is correlated with its writing quality, or because they are indicative of premises supporting a claim will have to be examined in further studies.

As a second finding, the influence of one half of the analyzed discourse-relation types on predicting review helpfulness has been shown to differ among two product categories, while the other half exhibited similar effects across both categories. Future comparisons of reviews clustered into more product categories and types (such as search and experience goods) will show whether this indicates the presence of certain mechanisms being common to the complete text type and others being sensitive to product types.

In total, the study presents a number of novel and partly unexpected findings and and opens up a range of research avenues. However, it also shows that a significant amount of additional research is needed to better understand the influence of discourse structure on review helpfulness.

\vfill


\begin{center}
Both code and data used for this study are freely available for research purposes at \url{https://github.com/s-go/th-review-helpfulness}.
\end{center}

\newpage
\begin{thebibliography}{9}

\bibitem[Almagrabi et al.(2015)]{Almagrabi2015} Almagrabi, H., Malibari, A., \& McNaught, J. (2015). A Survey of Quality Prediction of Product Reviews. In \textit{International Journal of Advanced Computer Science \& Applications}, 1(6), 49-58.

\bibitem[Eckle-Kohler et al.(2015)]{Eckle-Kohler2015} Eckle-Kohler, J., Kluge, R., \& Gurevych, I. (2015). On the Role of Discourse Markers for Discriminating Claims and Premises in Argumentative Discourse. In \textit{EMNLP}, 2236-2242.

\bibitem[Golly(2017)]{Golly2017} Golly, S. (2017). \textit{Predicting the Helpfulness of Product Reviews Using Discourse Relations.} Unpublished term paper. University of Potsdam, Potsdam, Germany. Retrieved from \url{https://github.com/s-go/cr-review-helpfulness/raw/master/docs/cr-review-helpfulness.pdf}

\bibitem[He and McAuley(2016)]{HeMcAuley2016} He, R., \& McAuley, J. (2016). Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering. In \textit{Proceedings of the 25th International Conference on World Wide Web}, International World Wide Web Conferences Steering Committee, 507-517.

\bibitem[Kamalski et al.(2008)]{Kamalski2008} Kamalski, J., Lentz, L., Sanders, T., \& Zwaan, R. A. (2008). The forewarning effect of coherence markers in persuasive discourse: Evidence from persuasion and processing. \textit{Discourse Processes}, 45, 545–579.

\bibitem[Kim et al.(2006)]{Kim2006} Kim, S. M., Pantel, P., Chklovski, T., \& Pennacchiotti, M. (2006). Automatically assessing review helpfulness. In \textit{Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing}, Association for Computational Linguistics, 423-430.

\bibitem[Lin et al.(2014)]{Lin2014} Lin, Z., Ng, H. T., \& Kan, M. Y. (2014). A PDTB-styled end-to-end discourse parser. In \textit{Natural Language Engineering}, 20(2), 151-184.

\bibitem[Liu et al.(2008)]{Liu2008} Liu, Y., Huang, X., An, A., \& Yu, X. (2008). Modeling and Predicting the Helpfulness of Online Reviews. In \textit{Data mining}, 2008. ICDM'08. Eighth IEEE international conference on Data mining, 443-452.

\bibitem[Mertz et al.(2014)]{Mertz2014} Mertz, M., Korfiatis, N., \& Zicari, R. V. (2014). Using Dependency Bigrams and Discourse Connectives for Predicting the Helpfulness of Online Reviews. In \textit{International Conference on Electronic Commerce and Web Technologies}, Springer International Publishing, 146-152.

\bibitem[Mudambi and Schuff(2010)]{MudambiSchuff2010} Mudambi, S. M., \& Schuff, D. (2010). What Makes a Helpful Online Review? A Study of Customer Reviews on Amazon.com. In \textit{MIS Quarterly}, 34(1), 185-200.

\bibitem[Nelson(1970)]{Nelson1970}Nelson, P. (1970). Information and consumer behavior. \textit{Journal of political economy}, 78(2), 311-329.

\bibitem[Prasad et al.(2008)]{Prasad2008}Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., Joshi, A., \& Webber, B. (2008). \textit{The Penn Discourse Treebank 2.0.} Paper presented at the 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakesh, Morocco.

\bibitem[Prasad et al.(2007)]{Prasad2007}Prasad, R., Miltsakaki, E., Dinesh, N., Lee, A., Joshi, A., Robaldo, L., \& Webber, B. L. (2007). \textit{The Penn Discourse Treebank 2.0 Annotation Manual.}

\bibitem[Stab and Gurevych(2014)]{StabGurevych2014}Stab, C., \& Gurevych, I. (2014). Identifying Argumentative Discourse Structures in Persuasive Essays. In \textit{EMNLP}, 46-56.

\end{thebibliography}

\end{document}
